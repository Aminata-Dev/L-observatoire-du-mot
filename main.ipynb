{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b26a377-83e4-4d8e-8517-59ddf81dd863",
   "metadata": {},
   "source": [
    "# L'observatoire du mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78188147-b1c6-4cc0-94cf-6508e38a100c",
   "metadata": {},
   "source": [
    "à faire :\n",
    "\n",
    "- ~~créer un sous-dossier pour les fichiers csv ?~~\n",
    "- les images ne fonctionnent pas dans github\n",
    "- créer la vérification synonymes.py et mettre à jour le rapport\n",
    "- ajout d'un readme -> intro de la fiche explicative\n",
    "- pourquoi le choix de scraper nos propores données ?\n",
    "- pourquoi pas de coloration syntaxique pour un chunc markdown sparql ?\n",
    "- docs -> ajouter un dossier wikidata\n",
    "- louis question -> que selectionne t on comme oeuvre dans Wikidata ?\n",
    "- json wikidata -> csv\n",
    "- à la fin nous integrerons les fichiers python en tant que fonctions d'un fichier utils.py\n",
    "- ajout traduction wikidata\n",
    "\n",
    "guidelines :\n",
    "- minimal core (dimension par dimension)\n",
    "\n",
    "questions :\n",
    "\n",
    "- Le rapport Excel dynamique doit contenir le plus de filtres possibles ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f37669",
   "metadata": {},
   "source": [
    "Liens importants : \n",
    "\n",
    "- cours : https://github.com/surybang/Reporting_openpyxl\n",
    "- template structure projet : https://github.com/surybang/template_usid0f\n",
    "- lien du repository : https://github.com/Aminata-Dev/L-observatoire-du-mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6d0a8-703a-40f7-8008-20c9527818e1",
   "metadata": {},
   "source": [
    "# Introduction – L’Observatoire du mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd221e-bf86-475f-834c-1dfcb9cda2c5",
   "metadata": {},
   "source": [
    "Qu’est-ce qu’un mot ? On croit souvent que comprendre un mot, c’est être capable de le définir. Pourtant, dans le langage ordinaire, les mots ne sont pas d’abord des objets de définition : ce sont des outils d’usage.\n",
    "\n",
    "Prenons pour exemple le mot « seum ». Se trouve-t-il dans le dictionnaire ? Peut-être. Mais même lorsqu’il y figure, la définition n’en fait que documenter un usage préexistant. Ce mot, comme tant d’autres, est d’abord appris par immersion, en l’entendant dans des situations précises puis en l’utilisant soi-même. On comprend un mot parce qu’on sait quand et comment l’utiliser – non parce qu’on est capable d’en réciter une définition. Les mots que l’on connait ne sont pas forcément des mots que l’on sait définir mais plutôt des mots que l’on sait utiliser. Comme l’écrivait Wittgenstein dans *Le Cahier Bleu* à ce sujet : « Nous sommes incapables de circonscrire clairement les concepts que nous utilisons ; non parce que nous ne connaissons pas leur vraie définition, mais parce qu'ils n'ont pas de vraie \"définition\". Supposer qu'il y en a nécessairement serait comme supposer que, à chaque fois que des enfants jouent avec un ballon, ils jouent en respectant des règles strictes.» (Wittgenstein, *Le Cahier bleu*, [25-26], trad. M. Goldberg et J. Sackur, Gallimard, p. 67-69).\n",
    "\n",
    "*L’Observatoire du mot* naît de ce constat : un mot est bien plus qu’une entrée dans un dictionnaire. Comprendre un mot, c’est plonger dans son usage vivant, ses contextes, ses résonances culturelles et sociales. L’utilisateur ne reçoit pas un portrait figé du mot de son choix, mais une matière vivante de l’explorer. L’Observatoire du mot est un outil hybride entre **dictionnaire augmenté** et cartographie culturelle pour toute personne curieuse de comprendre non seulement ce que signifie un mot mais également de connaître sa place dans le monde.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab062f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Objectifs et ambitions de *l'Observatoire du mot*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d7d89-046b-47cc-b41b-a55791d3422a",
   "metadata": {},
   "source": [
    "Notre objectif est de créer un programme Python permettant à l’utilisateur de saisir un mot, puis de générer un fichier Excel structuré, interactif et partageable. Ce fichier sera produit à l’aide de la librairie openpyxl. Voici la forme du ficher Excel que nous souhaitons produire :\n",
    "\n",
    "![Schéma](docs\\schema_dashboard_observatoire_du_mot.png)\n",
    "\n",
    "Cette maquette de **tableau de bord** est inférée de la liste suivante présentant les « dimensions » d’un mot que nous aimerions explorer et visualiser :\n",
    "\n",
    "- **Dimension sémantique** : définitions / étymologie / synonymes / antonymes / citations célèbres / traductions.\n",
    "- **Dimension culturelle** : apparition du mot dans les titres d’œuvres d’art\n",
    "- **Dimension sociale** via les réseaux sociaux\n",
    "  - Récupérer les tops tweet/threads/commentaires/titres de vidéos/hashtags contenant le mot\n",
    "  - Co-occurrences : avec quels autres mots notre mot se retrouve-t-il le plus ? \n",
    "  - **Dimension statistique** : \n",
    "    - évolution de la fréquence d’apparition du mot\n",
    "\t- Donner un score de popularité (étoiles)\n",
    "- **Dimension médiatique** via médias.\n",
    "  - Retrouver les articles de l'actualité contenant ce mot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ade560",
   "metadata": {},
   "source": [
    "# Réflexions techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df2ce9-6f24-4208-a072-92834f90ac57",
   "metadata": {},
   "source": [
    "Pour rendre l’expérience utilisateur fluide, nous envisageons d’ajouter une interface Streamlit dans laquelle l’utilisateur pourra entrer le mot à explorer.\n",
    "Le programme se chargera ensuite de lancer l’ensemble de la pipeline, sans nécessiter de modifications manuelles dans le code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa716d88-41e9-49ed-8d7c-e87fd81c96a3",
   "metadata": {},
   "source": [
    "## Les données du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97145b6e-0c05-4b5f-b885-ea85ed5598c6",
   "metadata": {},
   "source": [
    "Mais cela soulève une question centrale : comment structurer la récupération des données pour que tout fonctionne dans un ensemble cohérent ? En effet, nous devons collecter des données provenant de nombreuses **sources hétérogènes** pour donner vie à L’Observatoire du mot. Nous avons identifié quatre méthodes principales pour les collecter :\n",
    "\n",
    "- Le flux RSS qui est une manière standardisée de recevoir les derniers contenus publiés sur un site comme un fil d’actualité. Par exemple les flux RSS de journaux comme Le Monde ou Mediapart peuvent nous informer en temps réel des nouveaux articles contenant un mot. Le flux RSS permet de satisfaire la dimension médiatique et statistique.\n",
    "- L’utilisation d’API (Application Programming Interface), interfaces propres aux développeurs qui permettent de demander à des plateformes leurs données via un programme. Exemples : API de Genius, Spotify, IMDB, Reddit, YouTube, Twitter, forums… On demande par exemple \"tous les posts contenant le mot X\" et la plateforme renvoie une réponse structurée en JSON. L’utilisation d’API permet de satisfaire la dimension sociale, culturelle et statistique. L’utilisation d’API de grandes plateformes telles que citées ci-dessus donne également plus de crédibilité au projet car cette approche offre un comparatif qui suscitera l’intérêt de l’utilisateur et rend la dimension statistique plus fiable car ce sont des plateformes utilisées par des milliards d’utilisateurs.\n",
    "- Le scraping est une technique consistant à extraire automatiquement du contenu visible sur une page web. Nous pensons réaliser du scraping sur des pages web statiques comme Wikitionary, CNRTL, Gallica ou CRISCO pour satisfaire la dimension sémantique. Le scraping est un processus simple qui ne demande pas d’identifiants et de création d’applications comme le nécessite l’utilisation d’une API. \n",
    "- Nous pourrons également récupérer et exploiter des jeux de données existants récupérés sur internet (notamment Kaggle ou Projet Gutenberg), pour explorer la dimension culturelle (par exemple jeu de données référençant les titres d’œuvres les plus connus ou récupération de corpus littéraire) et sociale (par exemple top tweets ou forums archivés).\n",
    "\n",
    "Une fois les données collectées, un autre enjeu est de les structurer de manière cohérente malgré leur diversité. Chaque dimension identifiée (sémantique, culturelle, sociale, médiatique) devra respecter un schéma commun afin d’être aisément manipulable en Python et visualisable dans le fichier Excel final.\n",
    "\n",
    "Pour sécuriser notre projet et pouvoir tester l’observatoire sans être dépendants des aléas du scraping ou des limites d’API, nous prévoyons de **constituer un jeu de données de secours**. Ce fichier contiendra un échantillon représentatif pour chaque dimension (tweets les plus connus, titres d’œuvres célèbres, quelques articles de presse, …). Notre projet pourra donc être utilisé en mode déconnecté, ou en cas de saturation de services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbf013",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Interface streamlit\n",
    "\n",
    "Nous devons de passer par une interface Streamlit pour donner le choix de mot à l'utilisateur. Les questions qui se posent sont :\n",
    "\n",
    "- l'utilisateur doit il donner un mot existant (implique un fichier txt avec les mots français possibles)\n",
    "- faut il normaliser (oui),\n",
    "- gérer les bas de casse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cfb0a",
   "metadata": {},
   "source": [
    "# Programmation\n",
    "\n",
    "## Structure du programme\n",
    "\n",
    "Exemple :\n",
    "Nous créons un fichier Python par données. Le fichier main sera chargé de faire appel au module d'importation des données et ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071bc92",
   "metadata": {},
   "source": [
    "## Choix du mot\n",
    "\n",
    "```python\n",
    "mot_entree = input(\"\\nEntre un mot de ton choix\\n@> \")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14487db4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dimensions sémentique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca5afa",
   "metadata": {},
   "source": [
    "### Synonymes\n",
    "\n",
    "NP Louis : Pourquoi et comment le choix de ce site \n",
    "Ce site est simple à scraper, seule l'information essentielle y est affichée.\n",
    "\n",
    "Pour preuve, il suffit d'ajouter le mot du choix à l'url (et pas un code-index compliqué) pour accéder à la bonne page\n",
    "```python\n",
    "url = 'https://crisco4.unicaen.fr/des/synonymes/' + mot_entree\n",
    "```\n",
    "et commencer à parser le contenu html de la page\n",
    "```python\n",
    "page = requests.get(url)\n",
    "\n",
    "#bs4 permet de parser le contenu html d'une page \n",
    "soupe = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "```\n",
    "\n",
    "Nous souhaitons obtenir chaque synonyme associé au mot d'entrée et le score de popularité associé au synonyme. Ces éléments se présentent sous la forme suivante :\n",
    "\n",
    "![Tableau à scraper](docs/synonymes/tableau_synonymes_alambique.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cb606",
   "metadata": {},
   "source": [
    "![HTML derrière le tableau à scraper](docs/synonymes/inspection_tableau_synonymes_alambique.png)\n",
    "\n",
    "[Code source de la page](view-source:https://crisco4.unicaen.fr/des/synonymes/alambiqu%C3%A9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3633a7",
   "metadata": {},
   "source": [
    "Le tableau contient toutes les informations dont nous avons besoin pour commencer à scraper. Nous ciblons d'abord la balise `<table>` et nous recherchons le contenu des balises `<a href>` qui contiennent les synonymes. Une simple recherche du contenu des balises `<a href>` ira chercher tous les synonymes à l'ecran ailleurs que dans le tableau d'intérêt identifié plus haut et la somme des synonmymes sera supérieur au nombre de lignes du tableau. Nous avons donc adapté notre code de la manière suivante.\n",
    "\n",
    "```python\n",
    "synonymes = []\n",
    "\n",
    "#modèle : <a href=\"/des/synonymes/balle\">balle</a>\n",
    "for balise_a in soupe.find('table').find_all('a', href=True):\n",
    "    if \"/des/synonymes/\" in balise_a['href']:\n",
    "        synonyme = (balise_a.text).replace('\\xa0', '')\n",
    "        #print(synonyme)\n",
    "        synonymes.append(synonyme)\n",
    "#print(synonymes)\n",
    "```\n",
    "\n",
    "(\\d+): Les parenthèses () sont utilisées pour capturer un groupe. \\d correspond à n'importe quel chiffre (0-9), et le + signifie \"un ou plusieurs\". Donc, \\d+ correspond à une séquence d'un ou plusieurs chiffres. Cette partie de l'expression régulière capture la valeur numérique de la largeur.\n",
    "\n",
    "Ensuite nous observons que l'indication de taille de la barre affichée `width` nous permet d'obtenir l'indicateur de score de proximité. Voici le modèle d'une balise contenant l'information de taille de la barre : `<hr style=\"height:6px;width:14px;color:#4040C0;background-color:#4040C0;text-align:left;margin-left:0\">`. Nous utilisons donc une expression régulière afin de récupérer cette information et l'utiliser dans notre diagramme en bâtons.\n",
    "`r'width:(\\d+)px'`\n",
    "\n",
    "```python\n",
    "#modèle : <hr style=\"height:6px;width:14px;color:#4040C0;background-color:#4040C0;text-align:left;margin-left:0\">\n",
    "\n",
    "import re\n",
    "tailles_barre = []\n",
    "\n",
    "for hr in soupe.find_all('hr', style=True):\n",
    "    #print(hr[\"style\"])\n",
    "    \n",
    "    #extraction du nombre après \"width:\"\n",
    "    match = re.search(r'width:(\\d+)px', hr[\"style\"])\n",
    "\n",
    "    if match:\n",
    "        width = int(match.group(1))\n",
    "        #print(width)\n",
    "    \n",
    "    tailles_barre.append(width)\n",
    "```\n",
    "\n",
    "## Vérification\n",
    "Enfin, nous nous assurons que le nombre de score de proximité est bien le même que le nombre de synonyme trouvé grâce à l'instruction `assert len(tailles_barre) == len(synonymes)`.\n",
    "\n",
    "Pour éviter ce problème d'obtenir plus de synonymes et être certain de notre scraping (certains mots peuvent avoir une cinquantaine de synonymes recensés sur la page, dont plusieurs des doublons), nous effectuons une assertion pour vérifier que le nombre de synonymes trouvés et stockés dans la variable `synonymes` est bien le même que celui affiché sur site.\n",
    "\n",
    "```python\n",
    "for i in soupe.find_all('i', class_='titre'):\n",
    "\n",
    "    match_synonymes = re.search(r'(\\d+) synonymes', i.contents[0])\n",
    "\n",
    "    if match_synonymes:\n",
    "        nbr_synonymes = int(match_synonymes.group(1))\n",
    "        #print(nbr_synonymes)\n",
    "\n",
    "assert nbr_synonymes == len(synonymes)\n",
    "```\n",
    "Grâce à une l'inspection des éléments, nous sommes capables de récupérer et d'extraire le nombre associé au nombre de synonymes afin de vérifier nos résultats grâce à une expression régulière.\n",
    "\n",
    "NP : nous améliorons notre code pour qu'il soit capable de récupérer les antonymes d'une manière aussi propre que les synonymes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4748c85",
   "metadata": {},
   "source": [
    "Nous avons besoin de sauvegarder nos résultats afin de l'exploiter avec opepyxl. Nous décidons que chaque ficher tel que `synonymes.py` doit générer un fichier csv prêt à l'emploi pour l'utilisation par openpyxl.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e32e36",
   "metadata": {},
   "source": [
    "\n",
    "## Exportation CSV\n",
    "\n",
    "### Questions d'optimisations\n",
    "La question qui se pose est la suivante : vaut-il mieux utiliser **la librairie pandas ou la librarie csv** ? L'utilisation de la librarie csv disponible par défaut dans python permet de garder le programme ultra léger et plus rapide. En effet, la librarie pandas est plus lourde car elle inclut tout un écosystème data (ce qui implique des dépendances externes). Ces quelques milisecondes gagnées en utilisant une librarire ou une autre peuvent être cruciales si notre programme principal de génération du tableau de bord fait appel à plusieurs modules de scraping, requête API et flux RSS à la fois. Nous retenons tout de même la librairie pandas pour éviter de devoir inspecter les données à la main lorsque nous aurons à travailler avec données volumineuses comme les titres d'oeuvres d'art.\n",
    "\n",
    "### Demonstration\n",
    "Nous créons donc un ficher exportations_csv chargé d'exporter un dictionnaire python quelconque contenant les données de la manière suivante :\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def exporter_donnees_csv(donnees: dict[str, list], nom_fichier: str, index=False) -> None:\n",
    "    \"\"\"\n",
    "    Exporte un dictionnaire de données en fichier CSV.\n",
    "    \n",
    "    - donnees : dictionnaire {nom_colonne: liste_valeurs}\n",
    "    - nom_fichier : nom du fichier csv à créer (ex: \"synonymes.csv\")\n",
    "    \"\"\"\n",
    "\n",
    "    #création du chemin où sont stockés les données...\n",
    "    chemin_dossier = 'data'\n",
    "    chemin_complet = os.path.join(chemin_dossier, nom_fichier)\n",
    "    #... et création du sous-dossier data s'il n'existe pas\n",
    "    os.makedirs(chemin_dossier, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(donnees)\n",
    "    df.to_csv(chemin_complet, index=index)\n",
    "```\n",
    "\n",
    "Voici un exemple d'exportation de données dans un format csv prêt à l'emploi pour openpyxl :\n",
    "\n",
    "```python\n",
    "from exportation_csv import exporter_donnees_csv\n",
    "\n",
    "synonymes_csv = {\n",
    "    \"mot\": [mot_entree] *len(synonymes),\n",
    "    \"synonyme\": synonymes,\n",
    "    \"score_proximite_mot\": tailles_barre\n",
    "}\n",
    "\n",
    "exporter_donnees_csv(synonymes_csv, \"synonymes.csv\")\n",
    "```\n",
    "\n",
    "Chaque fichiers de données doit terminer par une instruction faisant appel à la fonction d'exportation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d105279-4221-46a8-8224-b3cda1e5d287",
   "metadata": {},
   "source": [
    "Nous n'avons plus qu'à récupérer la fiche lexicale du mot, qui concerne sa définition et étymologie. Beaucoup de sites possédant des cookies wall [(exemple)](https://www.larousse.fr/dictionnaires/francais/alambiqu%C3%A9/2015) sont bannis de notre recherche (pour des questions de privilégiés l'open data -> np : à rattacher avec la partie sur l'open source wikidata dans le diapo). Un site que nous utilisons souvent pour ses définitions complètes, ses multiples exemples et sa simplicité graphique est le site [Centre National de Ressources Textuelles et Lexicales (CNRTL)](https://www.cnrtl.fr/definition/alambiqu%C3%A9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c95f8-befd-480c-aaa0-f091a7095771",
   "metadata": {},
   "source": [
    "Prenons pour exemple le mot \"effervescent\".\n",
    "\n",
    "Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164d352",
   "metadata": {},
   "source": [
    "## Dimension culturelle\n",
    "\n",
    "Le but est d'obtenir la distribution des mots dans les titres d'oeuvres d'art afin d'obtenir la répartition du mot au sein du monde artistique et créer un camembert (secteur).\n",
    "\n",
    "J'ai eu à travailler avec Wikidata au sein de mon entreprise dans le cadre de la fiabilisation d'une base de données interne. Naturellement l'idée d'obtenir des informations de titre d'oeuvres d'art de manière rapide et structurée ne pouvait pas se passer de Wikidata.\n",
    " \n",
    "Voici une description de l'outil que nous allons utiliser :\n",
    " \n",
    "> Wikidata est une base de connaissances libre, collaborative et multilingue créée par la Wikimedia Foundation. Elle centralise des données structurées sur une grande variété de sujets : personnes, œuvres d’art, lieux, concepts, événements, etc. Contrairement à Wikipédia, qui s’adresse aux humains sous forme d’articles encyclopédiques, Wikidata organise l’information de manière à être facilement lisible et interrogeable par des machines. Elle constitue une pierre angulaire du Web sémantique.\n",
    "\n",
    "> Chaque élément dans Wikidata possède un identifiant unique (par exemple Q42 pour Douglas Adams) et est décrit à l’aide d’énoncés structurés : propriétés (P31 pour \"instance de\", P1476 pour \"titre\", etc.) et valeurs (autres entités ou chaînes de caractères). Ces données sont interconnectées, multilingues et peuvent être exploitées à grande échelle.\n",
    "\n",
    "> Pour interroger Wikidata, on utilise le langage SPARQL (SPARQL Protocol and RDF Query Language), un langage standard conçu pour extraire des informations de bases de données RDF (Resource Description Framework). SPARQL fonctionne comme un équivalent de SQL pour les données du Web sémantique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3983e6e-0d2e-4724-b679-bfa53ee64f0e",
   "metadata": {},
   "source": [
    "NP\n",
    "Voici une requête SPARQL similaire à notre objectif trouvée dans la documentation de Wikidata. Cette requête permet de [trouver tous les artistes dont le genre artistique contient le mot 'rock'](https://query.wikidata.org/#%23Artistes%20dont%20le%20genre%20artistique%20contient%20le%20mot%20%27rock%27%0ASELECT%20DISTINCT%20%3Fhuman%20%3FhumanLabel%0AWHERE%0A%7B%0A%20%20%20%20VALUES%20%3Fprofessions%20%7Bwd%3AQ177220%20wd%3AQ639669%7D%0A%20%20%20%20%3Fhuman%20wdt%3AP31%20wd%3AQ5%20.%0A%20%20%20%20%3Fhuman%20wdt%3AP106%20%3Fprofessions%20.%0A%20%20%20%20%3Fhuman%20wdt%3AP136%20%3Fgenre%20.%0A%20%20%20%20%3Fhuman%20wikibase%3Astatements%20%3Fstatementcount%20.%0A%20%20%20%20%3Fgenre%20rdfs%3Alabel%20%3FgenreLabel%20.%0A%20%20%20%20FILTER%20CONTAINS%28%3FgenreLabel%2C%20%22rock%22%29%20.%0A%20%20%20%20FILTER%20%28%3Fstatementcount%20%3E%2050%20%29%20.%0A%20%20%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20%7D%0A%7D%0AORDER%20BY%20%3FhumanLabel%0ALIMIT%2050)\n",
    "\n",
    "```sparql\n",
    "#Artistes dont le genre artistique contient le mot 'rock'\n",
    "SELECT DISTINCT ?human ?humanLabel\n",
    "WHERE\n",
    "{\n",
    "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
    "    ?human wdt:P31 wd:Q5 .\n",
    "    ?human wdt:P106 ?professions .\n",
    "    ?human wdt:P136 ?genre .\n",
    "    ?human wikibase:statements ?statementcount .\n",
    "    ?genre rdfs:label ?genreLabel .\n",
    "    FILTER CONTAINS(?genreLabel, \"rock\") .\n",
    "    FILTER (?statementcount > 50 ) .\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}\n",
    "ORDER BY ?humanLabel\n",
    "LIMIT 50\n",
    "```\n",
    "\n",
    "Voici le site que j'utilise lors de mon travail en entreprise : [Wikidata Query Service](https://query.wikidata.org/). En effet Wididata met à disposition une interface simple en ligne permettant\n",
    "\n",
    "- d'écrire des requêtes SPARQL (plusieurs requêtes d'exemple sont disponibles sur le site)\n",
    "- de visualiser les données scraper dans une table et\n",
    "- d'exporter les données en JSON ou CSV.\n",
    "\n",
    "De plus, Wikidata propose un [tutoriel simple](https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial/fr) pour comprendre comment créer une requête et obtenir les informations souhaitées sur la base de données Wikidata. Fait marrant, Wikidata introduit l'exemple d'une requête pour la recherche de toutes les oeuvres d'art au début de ce tutoriel pour expliquer un concept fondamental du schéma verbe - sujet -complément utilisé par les éléments de Wikidata.\n",
    "\n",
    "![Tutoriel Wikidata : Classes et Instances](docs/tutoriel_wikidata.png)\n",
    "\n",
    "> Lorsque j'ai écrit ceci (octobre 2016), cette requête retrouvait 2615 résultats.\n",
    "\n",
    "Le nombre d'élément ayant pour classe [\"oeuvre d'art\"](https://www.wikidata.org/wiki/Q838948) répertorié est maintenant de 34 319 le 4 juin et 34 320 le 5 juin 2025 !\n",
    "\n",
    "NP : Open Data mérite un crénau dans la soutenance. La **simplification de l'idée** pourrait être une bonne conclusion au projet. Pourquoi vouloir faire compliqué (API et scraping pour la dimension culturelle) quand un projet collaboratif s'acharne à créer une base de données complète et gratuite..\n",
    "\n",
    "Cependant et comme spécifié dans le tutoriel, requêter les oeuvres d'art n'est pas suffisant. Il faut également considérer les classes qui héritent de l'élément \"oeuvre d'art\", qu'ils soient enfants, petits-enfants ou petits petits enfants de l'élement oeuvre d'art. \n",
    "\n",
    "> Lorsque j'ai écrit ceci (octobre 2016), cette requête retrouvait 2615 résultats - évidemment, il y a plus d’œuvres d'art que cela ! Le problème est qu'il manque des éléments comme \"Autant en emporte le vent\", qui est seulement une instance de \"film\" et non de \"œuvre d'art\". \"film\" est une sous-classe d'\"œuvre d'art\", mais nous devons dire à SPARQL de prendre cela en compte lors de la recherche.\n",
    "\n",
    "```sparql\n",
    "WHERE\n",
    "{\n",
    "  ?oeuvre wdt:P31/wdt:P279* wd:Q838948. # >* instance de n'importe quelle sous-classe d'une œuvre d'art : mais trop d'oeuvres impossible à gérer\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "}\n",
    "```\n",
    "\n",
    "Mais cette requête pose problème. L'auteur de l'article exemple ecrit :\n",
    "> Je ne recommande pas d'exécuter cette requête. WDQS peut la gérer (tout juste), mais il est possible que votre navigateur se plante lors de l'affichage des résultats car ils sont très nombreux\n",
    "\n",
    "C'est exactement ce qu'il se passe de notre côté. Notre travail nécessite un découpage des requêtes car le résultat risque d'être trop grand et mettre trop de temps à charger malgré l'éfficacité du modèle des données Wikidata (np : intéressant pour les slides).\n",
    "\n",
    "\n",
    "En utilisant les requêtes montrées pécedemment (artistes dont le groupe contient le mot \"rock\" et la récupération de tous éléments instances d'oeuvre d'art), nous sommes de capables de produire la requête suivante. Nous choisissons de ne regarder que les XXX\n",
    "\n",
    "\n",
    "\n",
    "Les points de fins de ligne délimitent les instructions.\n",
    "\n",
    "Durant les première tentatives de cette requête (NP mettre un lien), nous nous attendions à voir _La nuit étoilée (Cyprès et Villages)_ de Van Gogh. Or nous ne trouvions pas le tableau dans la liste des résultats, élément qui nous a mis sur la piste que nous devions faire attention à la **sensibilité à la casse** et nous avons rectifons ce problème en remplaçant `sparql FILTER CONTAINS(?peintureLabel, \"nuit\").` par `sparql FILTER(CONTAINS(LCASE(?peintureLabel), \"nuit\")).`\n",
    "\n",
    "Mais cette requête a t elle du sens dans notre cas ? Compter les observations alors que Wikidata ne fait que réportorier des éléments. Bien qu'on peut considérer que Wikidata enregistre les peintures les plus connues, comme on l'a vu dans l'exemple du tutoriel, le nombre d'élément augmente au cours des contributions et donc au cours des années. En comptant le nombre de résultats, nous ne ferons qu'évaluer la taille de la base de données de Wikidata à un instant T, et non évaluer l'importance d'un mot dans les titres d'oeuvres d'art.\n",
    "\n",
    "Pour rémédier à cela il faudrait requêter les oeuvres les plus populaires (ne récupérer que celle dans le top 100 de chaque domaine artistique par exemple). Pour les oeuvres musicales, cela pourraient être toutes les chansons contenant le mot en question **et** ayant gagné un prix de musique. Cela permettrait de ne pas obtenir trop de résultats \n",
    "\n",
    "- De ne pas obtenir trop de résultats en filtrant dans la requête\n",
    "- D'avoir une véritable idée de la distribution d'un mot dans le monde artistique\n",
    "\n",
    "Il ne reste plus qu'à voir si les résultats ne seront pas trop désequilibré d'un art à l'autre : on peut par exemple supposer que la littérature sera surreprésenté par exemple.\n",
    "\n",
    "Nous passons donc par python. En effet, nous sommes capables grâce à la librairie requests d'obtenir les résultats d'une requête SPARQL sur Wikidata. Nous déléguons donc la requête au programme python puis nous récupérons le resultat dans un dictionnaire afin d'observer la répartition des classes d'éléments trouvés selon différents paramètres. En effet, beaucoup de combinaisons et de classes différents sont possibles (: récupération des eléménts de classe X, instance et sous-classe de X, sous-classe et sous-sous-classe de la classe Y,...).\n",
    "\n",
    "- L'utilisation de python permet de rendre la requête modulable grâce à l'utilisation de f-string. Un f-string en Python est une manière simple de créer des chaînes de caractères qui incluent des variables ou des expressions en les plaçant entre accolades dans une chaîne préfixée par f. Par exemple, f\"WHERE = {mot}!\" insère la valeur de la variable mot directement dans la chaîne.\n",
    "- L'utilisation de python et de la structure de données dictionnaire nous permet également de vérifier les résultats rapidement en effectuant la même opération : afficher la répartition des classes des éléments trouvés grâce à la requête.\n",
    "\n",
    "Voici le cheminement de pensée qui nous as améné à trouver la requête souitée :\n",
    "Par exemple, je veux obtenir dans mes résultats [Le Livre de Sable de Jorge Luis Borges](https://www.wikidata.org/wiki/Q20761845) lorsque je lance la requête avec le mot \"sable\", mais j'observe que chercher tous les éléments de Wikidata dont la classe est ou est sous-classe d'oeuvres d'art n'est pas suffisante.\n",
    "```sparql\n",
    "\n",
    "```\n",
    "\n",
    "Je prends donc la classe [**œuvre littéraire**](https://www.wikidata.org/wiki/Q7725634) à laquelle ce livre appartient et l'ajoute à la liste des classes pour laquelle on cherche des éléments.\n",
    "Pour le mot \"nuit\", je souhaite obtenir _La nuit étoilée de Van Gogh_, [_Le Songe d'une nuit d'été de William Shakespeare_](https://www.wikidata.org/wiki/Q104871) et [_Voyage au bout de la nuit de Louis-Ferdinand Céline_](https://www.wikidata.org/wiki/Q105102304) dans le même résultat, avec des types d'oeuvres différents. J'observe que le premier est une instance de peinture qui elle-même est sous-classe d'oeuvre d'art. Notre requête réussira donc à récupérer cette oeuvre. Le deuxième est une instance d'oeuvre dramatique qui elle-même est sous-classe de d'[oeuvre littéraire](https://www.wikidata.org/wiki/Q7725634) qui elle-meme ne sont pas sous-classe d'[oeuvre](https://www.wikidata.org/wiki/Q386724) mais des dérives : \n",
    "\n",
    "![Sous classe \"oeuvre litteraire\"](docs/wikidata_sous_classe_oeuvre_litteraire.png)\n",
    "\n",
    "Nous ajoutons donc aux côtés de la classe _oeuvre_ la classe [oeuvre littéraire](https://www.wikidata.org/wiki/Q7725634) dans notre recherche d'éléments. Cette méthode permet de récupérer [_Voyage au bout de la nuit de Louis-Ferdinand Céline_](https://www.wikidata.org/wiki/Q105102304) qui est une instance d'oeuvre littéraire. \n",
    "\n",
    "Ensuite, nous constatons qu'il n'y a aucun [film](https://www.wikidata.org/wiki/Q11424) dans la répartition de nos données. Nous ajoutons donc la [classe film](https://www.wikidata.org/wiki/Q11424) dans notre recherche. Il en va de même pour oeuvres musicales : nous cherchons une musique dans Wikidata, nous observons à quelle classe cet élément appartient et nous l'ajoutons à l'ensemble des classes cibles. Cependant, il n'y a pas beaucoup de chansons répértoriées. Pas autant que les albums de musique. Nous optons donc de choisir la classe \"album\" musical plutôt qu'oeuvres musicales.\n",
    "\n",
    "Ainsi, en inspectant nos données petit à petit, nous sommes capables de créer une requête équilibré et complète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d27439c8-d5de-4b88-94a8-a592857e2166",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Erreur lors de la requête SPARQL : 500",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mErreur lors de la requête SPARQL : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m donnees = \u001b[43mrechercher_oeuvres_wikidata\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnuit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#mot-test choisi = nuit\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mrechercher_oeuvres_wikidata\u001b[39m\u001b[34m(mot, langue)\u001b[39m\n\u001b[32m     30\u001b[39m response = requests.get(url, params={\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query}, headers=headers)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mErreur lors de la requête SPARQL : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[31mException\u001b[39m: Erreur lors de la requête SPARQL : 500"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def rechercher_oeuvres_wikidata(mot, langue=\"fr\"):\n",
    "    \"\"\"\n",
    "    Recherche les titres d'œuvres contenant un mot via Wikidata (SPARQL).\n",
    "    Retourne une liste de titres.\n",
    "    \"\"\"\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    \n",
    "    # Requête SPARQL : cherche des titres d'œuvres contenant le mot\n",
    "    #Nous rendons notre requête la plus modulable possible grâce à l'ajout de f-string\n",
    "    query = f\"\"\"SELECT DISTINCT ?oeuvre ?oeuvreLabel ?typeLabel\n",
    "        WHERE\n",
    "        {{\n",
    "          VALUES ?type {{ wd:Q3305213 wd:Q7725634 wd:Q11424 wd:Q482994 }}. #peinture, oeuvres littéraires, film, album musical /// wd:Q3305213 wd:Q7725634 wd:Q11424 wd:Q482994\n",
    "          ?oeuvre wdt:P31 ?type. #instance de l'ensemble de classe selectionné ci-dessus\n",
    "          \n",
    "          ?oeuvre rdfs:label ?oeuvreLabel. #Nous obtenons l'ensemble des libellés de nos oeuvres dans ?oeuvreLabel,\n",
    "          FILTER(LANG(?oeuvreLabel) = \"{langue}\"). #nous restreignons les libellés aux libéllés français\n",
    "          FILTER(CONTAINS(LCASE(?oeuvreLabel), \"{mot.lower()}\")). #...puis nous vérifions que les libéllés contiennent le mot en questions\n",
    "          \n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"{langue}\". }} #Nous créons les libéllés des variables\n",
    "        }}\n",
    "        LIMIT 500\n",
    "        \"\"\"\n",
    "    #Un f-string est une manière simple de créer des chaînes de caractères qui incluent des variables ou des expressions en les plaçant entre accolades dans une chaîne préfixée par f.\n",
    "\n",
    "    #envoie de la requête et récupération du résultat en json\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    response = requests.get(url, params={\"query\": query}, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur lors de la requête SPARQL : {response.status_code}\")\n",
    "    return response.json()\n",
    "\n",
    "donnees = rechercher_oeuvres_wikidata(\"nuit\") #mot-test choisi = nuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30c25a-1fe5-42b2-abda-4eb407d5bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_requete = donnees[\"results\"][\"bindings\"]\n",
    "\n",
    "#voici à quoi ressemble une observation de notre table résultat :\n",
    "print(table_requete[0])\n",
    "\n",
    "noms_oeuvres = []\n",
    "#Observons la répartition du type d'oeuvre\n",
    "repartition = {}\n",
    "for item in table_requete:\n",
    "    type_oeuvre = item[\"typeLabel\"][\"value\"]\n",
    "    repartition[type_oeuvre] = repartition.get(type_oeuvre, 0) + 1\n",
    "    noms_oeuvres.append(item['oeuvreLabel']['value'])\n",
    "\n",
    "#print(repartition)\n",
    "#print(sorted(repartition.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "top_5 = dict(sorted(repartition.items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(top_5)\n",
    "#La répartition est équilibrée\n",
    "\n",
    "#Nous testons nos résultats avec des oeuvres que nous connaissons. Ces tests ont permis de tester la validité de notre requête.\n",
    "print(\"Voyage au bout de la nuit\" in noms_oeuvres)\n",
    "print(\"La Nuit étoilée\" in noms_oeuvres)\n",
    "print(\"Le Songe d'une nuit d'été\" in noms_oeuvres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3749741-405c-4588-8614-66a901381eb4",
   "metadata": {},
   "source": [
    "NP pour Louis : il faut prendre que les tops pour le camambert sinon mal certains seront trop petits et non visibles (exemple : peinture vs poème)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938332a-8849-4dbc-bdc0-02df9079b0ac",
   "metadata": {},
   "source": [
    "le résultat des titres d'oeuvres de la requête, si filtrer avec un fichier de stopwords français, permettera de calculer les co-occurences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea773b48-a916-4e3e-873b-308a974ce725",
   "metadata": {},
   "source": [
    "# Dimensions sociale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1707b4-1ecc-4c08-8794-c4bd69d05726",
   "metadata": {},
   "source": [
    "En commençant nos recherches, nous nous rendons compte que dans beaucoup de cas, le scraping n'est pas légal et l'utilisation d'API restreinte sur de nombreuses plateformes tels que Twitter. Avoir la possibilité de récupérer des données depuis [Reddit](https://www.reddit.com/dev/api/) serait déjà beaucoup.\n",
    "\n",
    "Voilà notre modèle \n",
    "[Google Ngram Viewer](https://books.google.com/ngrams/graph?content=incandescent&year_start=1800&year_end=2022&corpus=fr&smoothing=3)\n",
    "qui présente l'évolution dans le temps de l'usage d'un mot. Nous souhaitons faire la même figure avec des réseaux sociaux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd45f5-1831-4e06-8f07-a6c0542b9bbf",
   "metadata": {},
   "source": [
    "## Twitter\n",
    "Twitter / X\n",
    "l’API v2 de Twitter propose une recherche sur mots-clés. Cependant l'accès nécessite un compte développeur validé et l’accès complet est payant, bien qu'un accès académique peut être demandé pour de la recherche. Nous optons donc pour une option plus simple : la récupération d'un jeu de données en ligne.\n",
    "Voici le jeu de données récupéré: [French Tweets For Sentiment Analysis - 1.5 million tweets in French and their sentiment - Kaggle](https://www.kaggle.com/datasets/hbaflast/french-twitter-sentiment-analysis)\n",
    "\n",
    "Voici les opérations réalisées afin d'otenir les statistiques que nous souhaitons obtenir à partir de ce jeu de données, à savoir :\n",
    "\n",
    "- Fréquence du mot dans les tweets et\n",
    "- Score de popularité\n",
    "\n",
    "Le calcul du score de popularité se calcule de la manière suivante.\n",
    "Si le mot choisi apparaissait autant de fois dans les tweets que le nombre de tweets de notre dataset, on considérait qu'il mérite le score maximum et son score de popularité est alors de 100%. \n",
    "\n",
    "Notre score de popularité se calcule donc de la manière suivante : $\\left(\\frac{\\text{fréquence\\_mot}}{\\text{nombre\\_tweets}}\\right) \\times 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc5ab2c-1bd3-4b95-9372-585343fee044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularité du mot 'nuit' : 2.46 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>souhaitant qu'elle passât cette nuit avec son ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>ouf! enfin tous en bus. si douloureux. mais la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>d'accord, je suis grillé ce soir. je suis deho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>mes téléphones ne vont pas passer toute la nui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>je ne bois jamais comme je l'ai fait la nuit d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49646</th>\n",
       "      <td>hahaa. manière d'être tom. au moins, vous ne s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49653</th>\n",
       "      <td>bonne nuit à tous. je suis désolé, je ne peux ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49673</th>\n",
       "      <td>belle nuit avec j'aimerais qu'elle ne me quitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49713</th>\n",
       "      <td>bonne nuit, juste au cas où</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49961</th>\n",
       "      <td>il est en retard ... 4 heures du matin, je pen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1230 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "11     souhaitant qu'elle passât cette nuit avec son ...\n",
       "74     ouf! enfin tous en bus. si douloureux. mais la...\n",
       "101    d'accord, je suis grillé ce soir. je suis deho...\n",
       "173    mes téléphones ne vont pas passer toute la nui...\n",
       "318    je ne bois jamais comme je l'ai fait la nuit d...\n",
       "...                                                  ...\n",
       "49646  hahaa. manière d'être tom. au moins, vous ne s...\n",
       "49653  bonne nuit à tous. je suis désolé, je ne peux ...\n",
       "49673  belle nuit avec j'aimerais qu'elle ne me quitt...\n",
       "49713                        bonne nuit, juste au cas où\n",
       "49961  il est en retard ... 4 heures du matin, je pen...\n",
       "\n",
       "[1230 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mot = \"nuit\"\n",
    "\n",
    "df = pd.read_csv('data/french_tweets_mini.csv') #le dataframe a été réduit à 50000 observations pour permettre le stockage sur Github\n",
    "\n",
    "nbr_tweets = df.shape[0]\n",
    "tweets_avec_mot = df[df[\"text\"].str.contains(mot.lower())]\n",
    "\n",
    "#print(tweets_avec_mot)\n",
    "frequence_mot = tweets_avec_mot.shape[0]\n",
    "\n",
    "#print(nbr_tweets, frequence_mot)\n",
    "\n",
    "#Si le mot choisi apparaissait autant de fois dans les tweets que le nombre de tweets de notre dataset, on considérait qu'il mérite le score maximum et son score de popularité serait 100%.\n",
    "score_popularite = ((frequence_mot/nbr_tweets)*100)\n",
    "\n",
    "print(f\"Popularité du mot '{mot}' : {round(score_popularite, 2)} %\")\n",
    "#exemple : Popularité du mot 'je' : 25.9 %\n",
    "\n",
    "from exportation_csv import exporter_donnees_csv\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\":[mot],\n",
    "    \"score\":[score_popularite]},\n",
    "    \"score_de_popularite.csv\"\n",
    ")\n",
    "\n",
    "tweets_avec_mot.to_csv(\"data/tweets_avec_mot.csv\")\n",
    "\n",
    "tweets_avec_mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9669c5-e583-427e-9814-f7204861073e",
   "metadata": {},
   "source": [
    "Après observation du dataset, beaucoup de personnes écrivent sans accent dans les tweets. Cela peut impacter la fréquence de mot trouvé selon la sensibilité à la casse ou le choix de l'encodage. Nous souhaitons donc normaliser le mot en entrée et le texte des tweets également avec la bibliothèque unidecode.\n",
    "\n",
    "> Unidecode transliterates any unicode string into the closest possible representation in ascii text\n",
    "\n",
    "Mais en essayant cette méthode, nous en venons à la conclusion que cette opération est beaucoup trop coûteuse en temps, bien plus que le simple fait de se passer de quelques mots pour le score de popularité comme montré ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8e28eb-1334-406f-ae64-d2a823fd5b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.29, 0.116)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twitter import recherche_twitter\n",
    "\n",
    "recherche_twitter(\"cle\"), recherche_twitter(\"clé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e57da-34d3-4769-8965-0b93ed8b57de",
   "metadata": {},
   "source": [
    "Limites de GitHub :\n",
    "GitHub refuse tout fichier > 100 Mo et déconseille les dépôts contenant plusieurs fichiers > 50 Mo. Nous créons donc une version minuscule de quelques Mo, passant de 1.5 millions de tweets à 50 000 tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
