{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b26a377-83e4-4d8e-8517-59ddf81dd863",
   "metadata": {},
   "source": [
    "# L'observatoire du mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78188147-b1c6-4cc0-94cf-6508e38a100c",
   "metadata": {},
   "source": [
    "à faire :\n",
    "\n",
    "- ~~créer un sous-dossier pour les fichiers csv ?~~\n",
    "- ~~les images ne fonctionnent pas dans github : / au lieu de \\\\~~\n",
    "- ~~créer la vérification synonymes.py et mettre à jour le rapport~~\n",
    "- ajout d'un readme -> intro de la fiche explicative\n",
    "- essayer l'interface web avec des mots qui n'existent pas\n",
    "- pourquoi le choix de scraper nos propores données ?\n",
    "- pourquoi pas de coloration syntaxique pour un chunc markdown sparql ?\n",
    "- docs -> ajouter un dossier wikidata\n",
    "- louis question -> que selectionne t on comme oeuvre dans Wikidata ?\n",
    "- json wikidata -> csv\n",
    "- à la fin nous integrerons les fichiers python en tant que fonctions d'un fichier utils.py\n",
    "- ajout traduction wikidata\n",
    "- interface web : avez-vous une application Reddit ?\n",
    "- tous les .py sont des fonctions\n",
    "\n",
    "guidelines :\n",
    "- minimal core (dimension par dimension)\n",
    "\n",
    "questions :\n",
    "\n",
    "- Le rapport Excel dynamique doit contenir le plus de filtres possibles ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f37669",
   "metadata": {},
   "source": [
    "Liens importants : \n",
    "\n",
    "- cours : https://github.com/surybang/Reporting_openpyxl\n",
    "- template structure projet : https://github.com/surybang/template_usid0f\n",
    "- lien du repository : https://github.com/Aminata-Dev/L-observatoire-du-mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6d0a8-703a-40f7-8008-20c9527818e1",
   "metadata": {},
   "source": [
    "# Introduction – L'Observatoire du mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd221e-bf86-475f-834c-1dfcb9cda2c5",
   "metadata": {},
   "source": [
    "Qu'est-ce qu'un mot ? On croit souvent que comprendre un mot, c'est être capable de le définir. Pourtant, dans le langage ordinaire, les mots ne sont pas d'abord des objets de définition : ce sont des outils d'usage.\n",
    "\n",
    "Prenons pour exemple le mot « seum ». Se trouve-t-il dans le dictionnaire ? Peut-être. Mais même lorsqu'il y figure, la définition n'en fait que documenter un usage préexistant. Ce mot, comme tant d'autres, est d'abord appris par immersion, en l'entendant dans des situations précises puis en l'utilisant soi-même. On comprend un mot parce qu'on sait quand et comment l'utiliser – non parce qu'on est capable d'en réciter une définition. Les mots que l'on connait ne sont pas forcément des mots que l'on sait définir mais plutôt des mots que l'on sait utiliser. Comme l'écrivait Wittgenstein dans *Le Cahier Bleu* à ce sujet : « Nous sommes incapables de circonscrire clairement les concepts que nous utilisons ; non parce que nous ne connaissons pas leur vraie définition, mais parce qu'ils n'ont pas de vraie \"définition\". Supposer qu'il y en a nécessairement serait comme supposer que, à chaque fois que des enfants jouent avec un ballon, ils jouent en respectant des règles strictes.» (Wittgenstein, *Le Cahier bleu*, [25-26], trad. M. Goldberg et J. Sackur, Gallimard, p. 67-69).\n",
    "\n",
    "*L'Observatoire du mot* naît de ce constat : un mot est bien plus qu'une entrée dans un dictionnaire. Comprendre un mot, c'est plonger dans son usage vivant, ses contextes, ses résonances culturelles et sociales. L'utilisateur ne reçoit pas un portrait figé du mot de son choix, mais une matière vivante de l'explorer. L'Observatoire du mot est un outil hybride entre **dictionnaire augmenté** et cartographie culturelle pour toute personne curieuse de comprendre non seulement ce que signifie un mot mais également de connaître sa place dans le monde.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab062f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Objectifs et ambitions de *l'Observatoire du mot*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d7d89-046b-47cc-b41b-a55791d3422a",
   "metadata": {},
   "source": [
    "Notre objectif est de créer un programme Python permettant à l'utilisateur de saisir un mot, puis de générer un fichier Excel structuré, interactif et partageable. Ce fichier sera produit à l'aide de la librairie openpyxl. Voici la forme du ficher Excel que nous souhaitons produire :\n",
    "\n",
    "![Schéma](docs/schema_dashboard_observatoire_du_mot.png)\n",
    "\n",
    "Cette maquette de **tableau de bord** est inférée de la liste suivante présentant les « dimensions » d'un mot que nous aimerions explorer et visualiser :\n",
    "\n",
    "- **Dimension sémantique** : définitions / étymologie / synonymes / antonymes / citations célèbres / traductions.\n",
    "- **Dimension culturelle** : apparition du mot dans les titres d'œuvres d'art\n",
    "- **Dimension sociale** via les réseaux sociaux\n",
    "  - Récupérer les tops tweet/threads/commentaires/titres de vidéos/hashtags contenant le mot\n",
    "  - Co-occurrences : avec quels autres mots notre mot se retrouve-t-il le plus ? \n",
    "  - **Dimension statistique** : \n",
    "    - évolution de la fréquence d'apparition du mot\n",
    "\t- Donner un score de popularité (étoiles)\n",
    "- **Dimension médiatique** via médias.\n",
    "  - Retrouver les articles de l'actualité contenant ce mot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ade560",
   "metadata": {},
   "source": [
    "# Réflexions techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df2ce9-6f24-4208-a072-92834f90ac57",
   "metadata": {},
   "source": [
    "Pour rendre l'expérience utilisateur fluide, nous envisageons d'ajouter une interface Streamlit dans laquelle l'utilisateur pourra entrer le mot à explorer.\n",
    "Le programme se chargera ensuite de lancer l'ensemble de la pipeline, sans nécessiter de modifications manuelles dans le code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa716d88-41e9-49ed-8d7c-e87fd81c96a3",
   "metadata": {},
   "source": [
    "## Les données du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97145b6e-0c05-4b5f-b885-ea85ed5598c6",
   "metadata": {},
   "source": [
    "Mais cela soulève une question centrale : comment structurer la récupération des données pour que tout fonctionne dans un ensemble cohérent ? En effet, nous devons collecter des données provenant de nombreuses **sources hétérogènes** pour donner vie à L'Observatoire du mot. Nous avons identifié quatre méthodes principales pour les collecter :\n",
    "\n",
    "- Le flux RSS qui est une manière standardisée de recevoir les derniers contenus publiés sur un site comme un fil d'actualité. Par exemple les flux RSS de journaux comme Le Monde ou Mediapart peuvent nous informer en temps réel des nouveaux articles contenant un mot. Le flux RSS permet de satisfaire la dimension médiatique et statistique.\n",
    "- L'utilisation d'API (Application Programming Interface), interfaces propres aux développeurs qui permettent de demander à des plateformes leurs données via un programme. Exemples : API de Genius, Spotify, IMDB, Reddit, YouTube, Twitter, forums… On demande par exemple \"tous les posts contenant le mot X\" et la plateforme renvoie une réponse structurée en JSON. L'utilisation d'API permet de satisfaire la dimension sociale, culturelle et statistique. L'utilisation d'API de grandes plateformes telles que citées ci-dessus donne également plus de crédibilité au projet car cette approche offre un comparatif qui suscitera l'intérêt de l'utilisateur et rend la dimension statistique plus fiable car ce sont des plateformes utilisées par des milliards d'utilisateurs.\n",
    "- Le scraping est une technique consistant à extraire automatiquement du contenu visible sur une page web. Nous pensons réaliser du scraping sur des pages web statiques comme Wikitionary, CNRTL, Gallica ou CRISCO pour satisfaire la dimension sémantique. Le scraping est un processus simple qui ne demande pas d'identifiants et de création d'applications comme le nécessite l'utilisation d'une API. \n",
    "- Nous pourrons également récupérer et exploiter des jeux de données existants récupérés sur internet (notamment Kaggle ou Projet Gutenberg), pour explorer la dimension culturelle (par exemple jeu de données référençant les titres d'œuvres les plus connus ou récupération de corpus littéraire) et sociale (par exemple top tweets ou forums archivés).\n",
    "\n",
    "Une fois les données collectées, un autre enjeu est de les structurer de manière cohérente malgré leur diversité. Chaque dimension identifiée (sémantique, culturelle, sociale, médiatique) devra respecter un schéma commun afin d'être aisément manipulable en Python et visualisable dans le fichier Excel final.\n",
    "\n",
    "Pour sécuriser notre projet et pouvoir tester l'observatoire sans être dépendants des aléas du scraping ou des limites d'API, nous prévoyons de **constituer un jeu de données de secours**. Ce fichier contiendra un échantillon représentatif pour chaque dimension (tweets les plus connus, titres d'œuvres célèbres, quelques articles de presse, …). Notre projet pourra donc être utilisé en mode déconnecté, ou en cas de saturation de services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b23b4-6605-4668-b8fc-18f45fa71e8e",
   "metadata": {},
   "source": [
    "# Programmation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9108607-9d37-4a7c-a125-9a6a4067f570",
   "metadata": {},
   "source": [
    "## Structure du programme\n",
    "\n",
    "Exemple :\n",
    "Nous créons un fichier Python par données. Le fichier main sera chargé de faire appel au module d'importation des données et ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071bc92",
   "metadata": {},
   "source": [
    "## Choix du mot\n",
    "\n",
    "```python\n",
    "mot_entree = input(\"\\nEntre un mot de ton choix\\n@> \")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2954d28-8b24-41e4-b2ff-b16741244d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mot = \"créativité\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14487db4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dimensions sémantique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e2802-2ee4-4956-8b90-6d1474c8fa34",
   "metadata": {},
   "source": [
    "### Synonymes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159f8f1-e17f-458e-a87e-39d8822cb715",
   "metadata": {},
   "source": [
    "NP Louis : Pourquoi et comment le choix de ce site \n",
    "Ce site est simple à scraper, seule l'information essentielle y est affichée.\n",
    "\n",
    "Pour preuve, il suffit d'ajouter le mot du choix à l'url (et pas un code-index compliqué) pour accéder à la bonne page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a22123-46e2-443f-8d96-9466f9c1ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://crisco4.unicaen.fr/des/synonymes/' + mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b4f0d-7637-423a-8589-3967a67ac97c",
   "metadata": {},
   "source": [
    "et commencer à parser le contenu html de la page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22e3e63-bee4-46c8-a634-2f721a6b934c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\amigr\\appdata\\roaming\\python\\python312\\site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\amigr\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\amigr\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502995f5-36cb-4784-9b89-33238afd1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "#bs4 permet de parser le contenu html d'une page \n",
    "soupe = BeautifulSoup(page.text, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296f9e2-613a-43f5-af55-7a60cfa0c2f3",
   "metadata": {},
   "source": [
    "Nous souhaitons obtenir chaque synonyme associé au mot d'entrée et le score de popularité associé au synonyme. Ces éléments se présentent sous la forme suivante :\n",
    "\n",
    "![Tableau à scraper](docs/synonymes/tableau_synonymes_alambique.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cb606",
   "metadata": {},
   "source": [
    "![HTML derrière le tableau à scraper](docs/synonymes/inspection_tableau_synonymes_alambique.png)\n",
    "\n",
    "[Code source de la page](view-source:https://crisco4.unicaen.fr/des/synonymes/alambiqu%C3%A9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a86cb-d817-4d7d-95c9-b049e60578ca",
   "metadata": {},
   "source": [
    "Ce tableau contient toutes les informations que nous avons besoin de scraper. Nous ciblons d'abord la balise `<table>` et nous recherchons le contenu des balises `<a href>` qui contiennent les synonymes.\n",
    "\n",
    "Il faut veiller à cibler la basise `<table>`, sans cela nos premiers scraping du contenu des balises `<a href>` allait chercher tous les synonymes à l'ecran ailleurs que dans le tableau d'intérêt identifié plus haut et la somme des synonmymes était supérieure au nombre de lignes du tableau. Nous avons donc adapté notre code de la manière suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8213c7d2-e40c-4f0f-a690-662f1528f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['besoin de nouveauté', 'inventivité', 'souffle', 'imagination']\n"
     ]
    }
   ],
   "source": [
    "synonymes = []\n",
    "\n",
    "#modèle : <a href=\"/des/synonymes/balle\">balle</a>\n",
    "for balise_a in soupe.find('table').find_all('a', href=True):\n",
    "    if \"/des/synonymes/\" in balise_a['href']:\n",
    "        synonyme = (balise_a.text).replace('\\xa0', '')\n",
    "        #print(synonyme)\n",
    "        synonymes.append(synonyme)\n",
    "        \n",
    "print(synonymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d487115-8dd6-4269-bfba-f08f84ecfb13",
   "metadata": {},
   "source": [
    "Ensuite nous observons que la taille de la barre affichée grâce à `width` nous permet d'obtenir l'indicateur de score de proximité. Voici le modèle d'une balise contenant l'information de taille de la barre : `<hr style=\"height:6px;width:14px;color:#4040C0;background-color:#4040C0;text-align:left;margin-left:0\">`. Nous utilisons donc une expression régulière afin de récupérer cette information et de l'utiliser dans notre diagramme en bâtons. L'expression régulière est la suivante : `r'width:(\\d+)px'`\n",
    "\n",
    "(\\d+): Les parenthèses () sont utilisées pour capturer un groupe. \\d correspond à n'importe quel chiffre (0-9), et le + signifie \"un ou plusieurs\". Donc, \\d+ correspond à une séquence d'un ou plusieurs chiffres. Cette partie de l'expression régulière capture la valeur numérique de la largeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62934c0a-280a-4d44-804e-b5568a45691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 50, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "#modèle : <hr style=\"height:6px;width:14px;color:#4040C0;background-color:#4040C0;text-align:left;margin-left:0\">\n",
    "\n",
    "import re\n",
    "tailles_barre = []\n",
    "\n",
    "for hr in soupe.find_all('hr', style=True):\n",
    "    #print(hr[\"style\"])\n",
    "    \n",
    "    #extraction du nombre après \"width:\"\n",
    "    match = re.search(r'width:(\\d+)px', hr[\"style\"])\n",
    "\n",
    "    if match:\n",
    "        width = int(match.group(1))\n",
    "        #print(width)\n",
    "    \n",
    "    tailles_barre.append(width)\n",
    "\n",
    "print(tailles_barre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb146fb4-3694-45de-89bd-23d740f4d9de",
   "metadata": {},
   "source": [
    "### Vérification de la fiabilité du scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b71301-68b6-47dc-8046-601cec968fc4",
   "metadata": {},
   "source": [
    "Enfin, nous nous assurons que le nombre de score de proximité est bien le même que le nombre de synonyme trouvé grâce à l'instruction `assert len(tailles_barre) == len(synonymes)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9e5346-2dd5-4e91-a5ac-f0b4c7022e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tailles_barre) == len(synonymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb44e2-b1c2-4b2e-bfad-2481a00bcd75",
   "metadata": {},
   "source": [
    "Pour éviter d'obtenir plus de synonymes et être certain de notre scraping (les mots ont une cinquantaine de synonymes recensés sur la page, dont plusieurs en doublons, doù le ciblage du tableau), nous effectuons une assertion pour vérifier que le nombre de synonymes trouvés et stockés dans la variable `synonymes` est bien le même que celui affiché sur site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc57872-b920-4c17-b331-3081123888da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in soupe.find_all('i', class_='titre'):\n",
    "\n",
    "    match_synonymes = re.search(r'(\\d+) synonymes', i.contents[0])\n",
    "\n",
    "    if match_synonymes:\n",
    "        nbr_synonymes = int(match_synonymes.group(1))\n",
    "        print(nbr_synonymes)\n",
    "\n",
    "assert nbr_synonymes == len(synonymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71748a61-e06e-4b3b-8f6b-5eae02d45890",
   "metadata": {},
   "source": [
    "NP : nous améliorerons notre code pour qu'il soit capable de récupérer les antonymes d'une manière aussi propre que les synonymes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4748c85",
   "metadata": {},
   "source": [
    "Enfin, nous avons besoin de sauvegarder nos résultats afin de l'exploiter avec openpyxl. Nous décidons que chaque fichier comme `synonymes.py` doit générer un fichier csv pour le stockage et prêt à l'emploi pour l'utilisation par openpyxl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd904cd6-58ac-451d-93d2-f542287c45b8",
   "metadata": {},
   "source": [
    "## Exportation CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f3f85-99f4-41ac-a560-eb29136eebd0",
   "metadata": {},
   "source": [
    "### Questions d'optimisations\n",
    "\n",
    "La question qui se pose est la suivante : vaut-il mieux utiliser la librairie pandas ou la librarie csv pour le projet pour travailler et exporter les données? L'utilisation de la librarie csv disponible par défaut dans python permet de garder le programme ultra léger et plus rapide. En effet, la librarie pandas est plus lourde car elle inclut tout un écosystème data (ce qui implique des dépendances externes). Ces quelques milisecondes gagnées en utilisant une librarire ou une autre peuvent être cruciales si notre programme principal de génération du tableau de bord fait appel à plusieurs modules de scraping, requête API et flux RSS à la fois. Nous retenons tout de même la librairie pandas pour éviter de devoir inspecter les données à la main lorsque nous aurons à travailler avec données volumineuses comme les titres d'oeuvres d'art.\n",
    "\n",
    "### Demonstration\n",
    "Nous créons donc un ficher exportations_csv chargé d'exporter un dictionnaire python quelconque contenant les données de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b153e3-67de-465c-9f8c-a2ba547bebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def exporter_donnees_csv(donnees: dict[str, list], nom_fichier: str, index=False) -> None:\n",
    "    \"\"\"\n",
    "    Exporte un dictionnaire de données en fichier CSV.\n",
    "    \n",
    "    - donnees : dictionnaire {nom_colonne: liste_valeurs}\n",
    "    - nom_fichier : nom du fichier csv à créer (ex: \"synonymes.csv\")\n",
    "    \"\"\"\n",
    "\n",
    "    #création du chemin où sont stockés les données...\n",
    "    chemin_dossier = 'data'\n",
    "    chemin_complet = os.path.join(chemin_dossier, nom_fichier)\n",
    "    #... et création du sous-dossier data s'il n'existe pas\n",
    "    os.makedirs(chemin_dossier, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(donnees)\n",
    "    df.to_csv(chemin_complet, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7dd79-36b4-4222-8c15-5b3e8bcc4cf3",
   "metadata": {},
   "source": [
    "Voici un exemple d'exportation de données dans un format csv prêt à l'emploi pour openpyxl :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319a5a88-fcb9-4013-b86c-ca54e26604e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymes_csv = {\n",
    "    \"mot\": [mot] *len(synonymes),\n",
    "    \"synonyme\": synonymes,\n",
    "    \"score_proximite_mot\": tailles_barre\n",
    "}\n",
    "\n",
    "exporter_donnees_csv(synonymes_csv, \"synonymes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aad765-c484-4cb2-8428-cd83e9ff7552",
   "metadata": {},
   "source": [
    "Chaque fichier python manipulant et traitant des données doit terminer par une instruction faisant appel à la fonction d'exportation. nous créons un module d'exportation et l'appelons dans les différents programmes de la manière suivante : \n",
    "```python \n",
    "from exportation_csv import exporter_donnees_csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620ca7c-d498-4aae-ad5b-e95ee1a607ef",
   "metadata": {},
   "source": [
    "Le code python permettant de récupérer les synonymes se trouve dans le fichier python `./synonymes.py`.\n",
    "\n",
    "Le code python permettant d'exporter un dictionnaire python en fichier csv se trouve dans le fichier `./exportation_csv.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed001c-53b9-48a9-8228-0e0b6fc12065",
   "metadata": {},
   "source": [
    "Il ne nous reste plus qu'à récupérer la fiche lexicale du mot. Ces éléments sont sa définition, sa prononciation et son étymologie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc03b06-5462-42f4-ad4f-0f6464dce78a",
   "metadata": {},
   "source": [
    "## Digression : l'Open Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9554c-f398-4ff1-a2c1-c168cc14bd86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Beaucoup de sites possédant des cookies wall [(exemple)](https://www.larousse.fr/dictionnaires/francais/alambiqu%C3%A9/2015) sont bannis de notre recherche. En effet, certaines plateformes, bien que riches en contenus, imposent des barrières à l'entrée (cookies wall, abonnements, API payantes, restrictions commerciales), limitant la possibilité d'exploration automatisée, de réutilisation ou de visualisation ouverte. Par exemple, des dictionnaires numériques grand public comme Larousse ou Le Robert verrouillent l'accès aux définitions via des dispositifs qui compliquent l'extraction de données à des fins de recherche ou d'analyse.\n",
    "\n",
    "En contraste, des ressources comme Wikidata, les flux RSS de la presse, ou encore des portails institutionnels comme le CNRTL (Centre National de Ressources Textuelles et Lexicales) incarnent l'esprit de l'open data. Ces plateformes favorisent la transparence de leurs structures de données, encouragent la réutilisation via des API ouvertes ou des formats interopérables et participent à la démocratisation de l'accès au savoir.\n",
    "\n",
    "Des plateformes comme le Centre National de Ressources Textuelles et Lexicales (CNRTL), issues de la recherche publique, offrent un accès libre à des définitions riches, étymologies, synonymes et exemples, sans publicité ni pistage. Dans la même optique, nous utiliserons Wikidata, une base de connaissances sémantique libre et collaborative, pour interroger dynamiquement nos œuvres à partir de mots en entrée. De la même façon, les flux RSS permettent de collecter légalement et en temps réel des titres et résumés d'articles d'actualité issus de médias variés, sans dépendre d'algorithmes opaques ou de systèmes de monétisation intrusifs.\n",
    "\n",
    "Ce type de ressource devient essentiel pour bâtir des outils linguistiques, culturels ou éducatifs à destination du grand public.\n",
    "\n",
    "Ce choix de l'open data est donc à la fois une nécessité technique (ne pas dépendre d'écosystèmes fermés comme Twitter, dont les API sont devenues depuis peu inaccessibles) et une opportunité méthodologique (simplicité d'utilisation des données et formats standards) et un engagement politique pour une culture des communs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e319b-1196-4409-ab0b-98e31b8ec0d4",
   "metadata": {},
   "source": [
    "## Fiche lexicale\n",
    "\n",
    "Nous utilisons pour base de la création de fichiers csv constituant notre fiche lexicale pour leurs définitions complètes, leurs multiples exemples et leurs simplicité d'interface sans publicité les sites\n",
    "- [Centre National de Ressources Textuelles et Lexicales (CNRTL)](https://www.cnrtl.fr/definition/incandescent).\n",
    "- [Wikitionary](https://fr.wiktionary.org/wiki/incandescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c95f8-befd-480c-aaa0-f091a7095771",
   "metadata": {},
   "source": [
    "Nous créons un fichier python dédié à la génération d'une fiche lexicale. Pour se faire, nous extrayons les données des deux sites cités précédemment pour obtenir une fiche lexicale complète.\n",
    "\n",
    "Voici notre code de scraping, qui suit la même logique que la récupération des synonymes, en gardant en tête qu'un bon scraping ne se fait pas avant d'avoir étudié le code source de la page en profondeur et avant d'avoir effectué les tests sur plusieurs cas pour repérer les erreurs de scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a8dd65-4c11-4823-96ca-fd9bab11f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Définition :\n",
      "- Capacité, pouvoir qu'a un individu de créer, c'est-à-dire d'imaginer et de réaliser quelque chose de nouveau.\n",
      "- Capacité de découvrir une solution nouvelle, originale, à un problème donné.\n",
      "- Mise en œuvre par un individu.\n",
      "- Mise en œuvre collective de ce pouvoir par un ensemble d'individus.\n",
      "- Capacité de comprendre et de produire un nombre indéfini de nouveaux énoncés (d'apr. Mounin 1974).\n",
      "- Pouvoir d'enrichir le lexique en recourant aux divers procédés de dérivation\n"
     ]
    }
   ],
   "source": [
    "#CNRTL\n",
    "url = f\"https://www.cnrtl.fr/definition/{mot}\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "##### Définitions\n",
    "definitions = []\n",
    "for span in soup.find_all(\"span\", class_=\"tlf_cdefinition\"):\n",
    "    definitions.append(span.text)\n",
    "    \n",
    "print(\"\\nDéfinition :\")\n",
    "for d in definitions:\n",
    "    print(\"-\", d)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot] * len(definitions),\n",
    "    \"définition\":definitions},\n",
    "    \"definitions.csv\"\n",
    ")\n",
    "\n",
    "##### Étymologie\n",
    "etymologies = []\n",
    "for span in soup.find_all(\"span\", class_=\"tlf_ety\"):\n",
    "    etymologies.append(span.text)\n",
    "#print(etymologies)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot] * len(etymologies),\n",
    "    \"etymologie\":etymologies},\n",
    "    \"etymologies.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73576a7b-c359-4a10-a8ba-862177580cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Citations :\n",
      "- La psychologie de la vieillesse n’est encore qu’esquissée. La créativité, qui est prospection dans le temps, s’épuise.\n",
      "- Vos supérieurs aiment votre créativité et vous pourriez obtenir une prime. Youhou !\n",
      "- La créativité, c’est l’aptitude à avoir de bonnes idées de solutions de façon répétées, si possibles nouvelles (inattendues aux yeux des autres).\n",
      "- Des scientifiques ont découvert une série de gènes liés à la créativité qui pourraient avoir donné à l'Homo sapiens un avantage significatif sur l'Homo neanderthalensis, lui permettant d'éviter l'extinction.\n",
      "- Ainsi, la créativité est la faculté de composer, de construire, à partir d’éléments déconstruits, un concept, en effectuant sa synthèse. C’est une succession ou une alternance de moments créatifs (d’imagination) et créateurs (de création).\n",
      "\n",
      "Prononciation :  \\kʁe.a.ti.vi.te\\\n"
     ]
    }
   ],
   "source": [
    "#Wikitionary\n",
    "url = f\"https://fr.wiktionary.org/wiki/{mot}\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "##### Citations\n",
    "#modèle : <bdi lang=\"fr\" class=\"lang-fr\"><i>Le lendemain, ils ne se voyaient pas. Les couples restaient enfermés chez eux, à la diète, écœurés, abusant de cafés noirs et de cachets <b>effervescents</b>.</i></bdi>\n",
    "citations = []\n",
    "for bdi in soup.find_all(\"bdi\", lang=\"fr\", class_=\"lang-fr\"):\n",
    "    if bdi.get(\"about\", \"\") != \"#mwt10\": #sinon prends des choses autres que des défintions dans la page\n",
    "        try:\n",
    "            citation = bdi.find(\"i\").text\n",
    "            #print(citation)\n",
    "            citations.append(citation)\n",
    "        except:pass #si pas de citations\n",
    "\n",
    "print(\"\\nCitations :\")\n",
    "for c in citations:\n",
    "    print(\"-\", c)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot] * len(citations),\n",
    "    \"citation\":citations},\n",
    "    \"citations.csv\"\n",
    ")\n",
    "\n",
    "##### Prononciation API\n",
    "#modèle prononciation : <span class=\"API\" title=\"Prononciation API\">\\e.fɛʁ.ve.sɑ̃\\</span></a>\n",
    "prononciation = mot\n",
    "for span in soup.find(\"span\", class_=\"API\",title=\"Prononciation API\"): #le premier est le français, les autres concernent les traductions\n",
    "    #print(span)\n",
    "    prononciation = span.text\n",
    "print(\"\\nPrononciation : \", prononciation)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot],\n",
    "    \"prononciation\":[prononciation]},\n",
    "    \"prononciation.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46879701-1a1c-4c96-9fdd-77a553e925c9",
   "metadata": {},
   "source": [
    "Le code entier se trouve dans le fichier `./fiche_lexicale.py`. Pour extraire la fiche lexicale d'un mot, il faut appeler le fonction `recup_fiche_lexicale()` du fichier qui prend pour seul argument le mot d'entrée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd937d04-0c51-44b4-9ebd-07bc6b20acbe",
   "metadata": {},
   "source": [
    "## Dimension culturelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77353ee9-6917-42de-bfef-9e220fee5640",
   "metadata": {},
   "source": [
    "Le but est d'obtenir la distribution du mot d'entrée dans les titres d'oeuvres d'art afin d'en observer la répartition au sein du monde artistique. Nous souhaitons ensuite en faire un camembert (diagramme en secteurs).\n",
    "\n",
    "J'ai eu à travailler avec Wikidata au sein de mon entreprise dans le cadre de la fiabilisation d'une base de données interne. Naturellement l'idée d'obtenir des informations de titre d'oeuvres d'art de manière rapide et structurée ne pouvait pas se passer de Wikidata.\n",
    " \n",
    "Voici une description de l'outil que nous allons utiliser :\n",
    " \n",
    "> Wikidata est une base de connaissances libre, collaborative et multilingue créée par la Wikimedia Foundation. Elle centralise des données structurées sur une grande variété de sujets : personnes, œuvres d'art, lieux, concepts, événements, etc. Contrairement à Wikipédia, qui s'adresse aux humains sous forme d'articles encyclopédiques, Wikidata organise l'information de manière à être facilement lisible et interrogeable par des machines. Elle constitue une pierre angulaire du Web sémantique.\n",
    "\n",
    "> Chaque élément dans Wikidata possède un identifiant unique (par exemple Q42 pour Douglas Adams) et est décrit à l'aide d'énoncés structurés : propriétés (P31 pour \"instance de\", P1476 pour \"titre\", etc.) et valeurs (autres entités ou chaînes de caractères). Ces données sont interconnectées, multilingues et peuvent être exploitées à grande échelle.\n",
    "\n",
    "> Pour interroger Wikidata, on utilise le langage SPARQL (SPARQL Protocol and RDF Query Language), un langage standard conçu pour extraire des informations de bases de données RDF (Resource Description Framework). SPARQL fonctionne comme un équivalent de SQL pour les données du Web sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac1e08-1953-4694-ac95-04c8f7931c77",
   "metadata": {},
   "source": [
    "Voici un exemple de requête SPARQL similaire à notre objectif et trouvée dans la documentation de Wikidata. Cette requête permet de [trouver tous les artistes dont le genre artistique contient le mot 'rock'](https://query.wikidata.org/#%23Artistes%20dont%20le%20genre%20artistique%20contient%20le%20mot%20%27rock%27%0ASELECT%20DISTINCT%20%3Fhuman%20%3FhumanLabel%0AWHERE%0A%7B%0A%20%20%20%20VALUES%20%3Fprofessions%20%7Bwd%3AQ177220%20wd%3AQ639669%7D%0A%20%20%20%20%3Fhuman%20wdt%3AP31%20wd%3AQ5%20.%0A%20%20%20%20%3Fhuman%20wdt%3AP106%20%3Fprofessions%20.%0A%20%20%20%20%3Fhuman%20wdt%3AP136%20%3Fgenre%20.%0A%20%20%20%20%3Fhuman%20wikibase%3Astatements%20%3Fstatementcount%20.%0A%20%20%20%20%3Fgenre%20rdfs%3Alabel%20%3FgenreLabel%20.%0A%20%20%20%20FILTER%20CONTAINS%28%3FgenreLabel%2C%20%22rock%22%29%20.%0A%20%20%20%20FILTER%20%28%3Fstatementcount%20%3E%2050%20%29%20.%0A%20%20%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20%7D%0A%7D%0AORDER%20BY%20%3FhumanLabel%0ALIMIT%2050)\n",
    "\n",
    "```sparql\n",
    "#Artistes dont le genre artistique contient le mot 'rock'\n",
    "SELECT DISTINCT ?human ?humanLabel\n",
    "WHERE\n",
    "{\n",
    "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
    "    ?human wdt:P31 wd:Q5 .\n",
    "    ?human wdt:P106 ?professions .\n",
    "    ?human wdt:P136 ?genre .\n",
    "    ?human wikibase:statements ?statementcount .\n",
    "    ?genre rdfs:label ?genreLabel .\n",
    "    FILTER CONTAINS(?genreLabel, \"rock\") .\n",
    "    FILTER (?statementcount > 50 ) .\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}\n",
    "ORDER BY ?humanLabel\n",
    "LIMIT 50\n",
    "```\n",
    "\n",
    "Voici le site que j'utilise lors de mon travail en entreprise : [Wikidata Query Service](https://query.wikidata.org/). En effet Wididata met à disposition une interface simple en ligne permettant\n",
    "\n",
    "- d'écrire des requêtes SPARQL (plusieurs requêtes d'exemple sont disponibles sur le site);\n",
    "- de visualiser les données scraper dans une table et de faire des recherches de valeurs dedans;\n",
    "- d'exporter les données en JSON ou CSV.\n",
    "\n",
    "De plus, Wikidata propose un [tutoriel simple](https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial/fr) pour comprendre comment créer une requête pour obtenir les informations souhaitées grâce à la base de données Wikidata. Fait marrant, Wikidata commence son tutoriel par l'exemple d'une requête pour la recherche de toutes les oeuvres d'art pour expliquer un concept fondamental du schéma verbe - sujet -complément utilisé par la base de données Wikidata.\n",
    "\n",
    "![Tutoriel Wikidata : Classes et Instances](docs/tutoriel_wikidata.png)\n",
    "\n",
    "> Lorsque j'ai écrit ceci (octobre 2016), cette requête retournait 2615 résultats.\n",
    "\n",
    "Le nombre d'éléments ayant pour classe [\"oeuvre d'art\"](https://www.wikidata.org/wiki/Q838948) répertorié est maintenant de 34 319 le 4 juin et 34 320 le 5 juin 2025 !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c0143-5d02-401a-9050-332cd84e781e",
   "metadata": {},
   "source": [
    "Cependant et comme spécifié dans le tutoriel, requêter les oeuvres d'art n'est pas suffisant. Il faut également considérer les classes qui héritent de l'élément \"oeuvre d'art\", qu'ils soient enfants, petits-enfants ou petits petits enfants de l'élement oeuvre d'art. \n",
    "> Lorsque j'ai écrit ceci (octobre 2016), cette requête retrouvait 2615 résultats - évidemment, il y a plus d'œuvres d'art que cela ! Le problème est qu'il manque des éléments comme \"Autant en emporte le vent\", qui est seulement une instance de \"film\" et non de \"œuvre d'art\". \"film\" est une sous-classe d'\"œuvre d'art\", mais nous devons dire à SPARQL de prendre cela en compte lors de la recherche.\n",
    "\n",
    "La requête associée est la suivante :\n",
    "```sparql\n",
    "WHERE\n",
    "{\n",
    "  ?oeuvre wdt:P31/wdt:P279* wd:Q838948. # >* instance de n'importe quelle sous-classe d'une œuvre d'art : mais trop d'oeuvres -> la requête ne se termine pas\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "}\n",
    "```\n",
    "\n",
    "Mais cette requête pose problème. L'auteur de l'article exemple ecrit :\n",
    "> Je ne recommande pas d'exécuter cette requête. WDQS peut la gérer (tout juste), mais il est possible que votre navigateur se plante lors de l'affichage des résultats car ils sont très nombreux.\n",
    "\n",
    "C'est exactement ce qu'il se passe de notre côté.\n",
    "\n",
    "![Limite du temps de requête atteinte](docs/wikidata_limite_temps_atteinte.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffd4f0-521b-4152-a98f-80bf37610af6",
   "metadata": {},
   "source": [
    "Nous devons donc procéder à un découpage ...\n",
    "NP : solution ?\n",
    "\n",
    "En utilisant les requêtes montrées pécedemment (artistes dont le groupe contient le mot \"rock\" et la récupération de tous éléments instances d'oeuvre d'art), nous sommes de capables de produire la requête suivante. Nous choisissons de ne regarder que les XXX\n",
    "\n",
    "\n",
    "Les points de fins de ligne délimitent les instructions.\n",
    "\n",
    "Durant les première tentatives de cette requête (NP mettre un lien), nous nous attendions à voir _La nuit étoilée (Cyprès et Villages)_ de Van Gogh. Or nous ne trouvions pas le tableau dans la liste des résultats, élément qui nous a mis sur la piste que nous devions faire attention à la **sensibilité à la casse** et nous avons rectifons ce problème en remplaçant `sparql FILTER CONTAINS(?peintureLabel, \"nuit\").` par `sparql FILTER(CONTAINS(LCASE(?peintureLabel), \"nuit\")).`\n",
    "\n",
    "Mais cette requête a t elle du sens dans notre cas ? Compter les observations alors que Wikidata ne fait que réportorier des éléments. Bien qu'on peut considérer que Wikidata enregistre les peintures les plus connues, comme on l'a vu dans l'exemple du tutoriel, le nombre d'élément augmente au cours des contributions et donc au cours des années. En comptant le nombre de résultats, nous ne ferons qu'évaluer la taille de la base de données de Wikidata à un instant T, et non évaluer l'importance d'un mot dans les titres d'oeuvres d'art.\n",
    "\n",
    "Pour rémédier à cela il faudrait requêter les oeuvres les plus populaires (ne récupérer que celle dans le top 100 de chaque domaine artistique par exemple). Pour les oeuvres musicales, cela pourraient être toutes les chansons contenant le mot en question **et** ayant gagné un prix de musique. Cela permettrait de ne pas obtenir trop de résultats \n",
    "\n",
    "- De ne pas obtenir trop de résultats en filtrant dans la requête\n",
    "- D'avoir une véritable idée de la distribution d'un mot dans le monde artistique\n",
    "\n",
    "Il ne reste plus qu'à voir si les résultats ne seront pas trop désequilibré d'un art à l'autre : on peut par exemple supposer que la littérature sera surreprésenté par exemple.\n",
    "\n",
    "Nous passons donc par python. En effet, nous sommes capables grâce à la librairie requests d'obtenir les résultats d'une requête SPARQL sur Wikidata. Nous déléguons donc la requête au programme python puis nous récupérons le resultat dans un dictionnaire afin d'observer la répartition des classes d'éléments trouvés selon différents paramètres. En effet, beaucoup de combinaisons et de classes différents sont possibles (: récupération des eléménts de classe X, instance et sous-classe de X, sous-classe et sous-sous-classe de la classe Y,...).\n",
    "\n",
    "- L'utilisation de python permet de rendre la requête modulable grâce à l'utilisation de f-string. Un f-string en Python est une manière simple de créer des chaînes de caractères qui incluent des variables ou des expressions en les plaçant entre accolades dans une chaîne préfixée par f. Par exemple, f\"WHERE = {mot}!\" insère la valeur de la variable mot directement dans la chaîne.\n",
    "- L'utilisation de python et de la structure de données dictionnaire nous permet également de vérifier les résultats rapidement en effectuant la même opération : afficher la répartition des classes des éléments trouvés grâce à la requête.\n",
    "\n",
    "Voici le cheminement de pensée qui nous as améné à trouver la requête souitée :\n",
    "Par exemple, je veux obtenir dans mes résultats [Le Livre de Sable de Jorge Luis Borges](https://www.wikidata.org/wiki/Q20761845) lorsque je lance la requête avec le mot \"sable\", mais j'observe que chercher tous les éléments de Wikidata dont la classe est ou est sous-classe d'oeuvres d'art n'est pas suffisante.\n",
    "```sparql\n",
    "\n",
    "```\n",
    "\n",
    "Je prends donc la classe [**œuvre littéraire**](https://www.wikidata.org/wiki/Q7725634) à laquelle ce livre appartient et l'ajoute à la liste des classes pour laquelle on cherche des éléments.\n",
    "Pour le mot \"nuit\", je souhaite obtenir _La nuit étoilée de Van Gogh_, [_Le Songe d'une nuit d'été de William Shakespeare_](https://www.wikidata.org/wiki/Q104871) et [_Voyage au bout de la nuit de Louis-Ferdinand Céline_](https://www.wikidata.org/wiki/Q105102304) dans le même résultat, avec des types d'oeuvres différents. J'observe que le premier est une instance de peinture qui elle-même est sous-classe d'oeuvre d'art. Notre requête réussira donc à récupérer cette oeuvre. Le deuxième est une instance d'oeuvre dramatique qui elle-même est sous-classe de d'[oeuvre littéraire](https://www.wikidata.org/wiki/Q7725634) qui elle-meme ne sont pas sous-classe d'[oeuvre](https://www.wikidata.org/wiki/Q386724) mais des dérives : \n",
    "\n",
    "![Sous classe \"oeuvre litteraire\"](docs/wikidata_sous_classe_oeuvre_litteraire.png)\n",
    "\n",
    "Nous ajoutons donc aux côtés de la classe _oeuvre_ la classe [oeuvre littéraire](https://www.wikidata.org/wiki/Q7725634) dans notre recherche d'éléments. Cette méthode permet de récupérer [_Voyage au bout de la nuit de Louis-Ferdinand Céline_](https://www.wikidata.org/wiki/Q105102304) qui est une instance d'oeuvre littéraire. \n",
    "\n",
    "Ensuite, nous constatons qu'il n'y a aucun [film](https://www.wikidata.org/wiki/Q11424) dans la répartition de nos données. Nous ajoutons donc la [classe film](https://www.wikidata.org/wiki/Q11424) dans notre recherche. Il en va de même pour oeuvres musicales : nous cherchons une musique dans Wikidata, nous observons à quelle classe cet élément appartient et nous l'ajoutons à l'ensemble des classes cibles. Cependant, il n'y a pas beaucoup de chansons répértoriées. Pas autant que les albums de musique. Nous optons donc de choisir la classe \"album\" musical plutôt qu'oeuvres musicales.\n",
    "\n",
    "Ainsi, en inspectant nos données petit à petit, nous sommes capables de créer une requête équilibré et complète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d27439c8-d5de-4b88-94a8-a592857e2166",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Erreur lors de la requête SPARQL : 500",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mErreur lors de la requête SPARQL : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m donnees = \u001b[43mrechercher_oeuvres_wikidata\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnuit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#mot-test choisi = nuit\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mrechercher_oeuvres_wikidata\u001b[39m\u001b[34m(mot, langue)\u001b[39m\n\u001b[32m     30\u001b[39m response = requests.get(url, params={\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query}, headers=headers)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mErreur lors de la requête SPARQL : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[31mException\u001b[39m: Erreur lors de la requête SPARQL : 500"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def rechercher_oeuvres_wikidata(mot, langue=\"fr\"):\n",
    "    \"\"\"\n",
    "    Recherche les titres d'œuvres contenant un mot via Wikidata (SPARQL).\n",
    "    Retourne une liste de titres.\n",
    "    \"\"\"\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    \n",
    "    # Requête SPARQL : cherche des titres d'œuvres contenant le mot\n",
    "    #Nous rendons notre requête la plus modulable possible grâce à l'ajout de f-string\n",
    "    query = f\"\"\"SELECT DISTINCT ?oeuvre ?oeuvreLabel ?typeLabel\n",
    "        WHERE\n",
    "        {{\n",
    "          VALUES ?type {{ wd:Q3305213 wd:Q7725634 wd:Q11424 wd:Q482994 }}. #peinture, oeuvres littéraires, film, album musical /// wd:Q3305213 wd:Q7725634 wd:Q11424 wd:Q482994\n",
    "          ?oeuvre wdt:P31 ?type. #instance de l'ensemble de classe selectionné ci-dessus\n",
    "          \n",
    "          ?oeuvre rdfs:label ?oeuvreLabel. #Nous obtenons l'ensemble des libellés de nos oeuvres dans ?oeuvreLabel,\n",
    "          FILTER(LANG(?oeuvreLabel) = \"{langue}\"). #nous restreignons les libellés aux libéllés français\n",
    "          FILTER(CONTAINS(LCASE(?oeuvreLabel), \"{mot.lower()}\")). #...puis nous vérifions que les libéllés contiennent le mot en questions\n",
    "          \n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"{langue}\". }} #Nous créons les libéllés des variables\n",
    "        }}\n",
    "        LIMIT 500\n",
    "        \"\"\"\n",
    "    #Un f-string est une manière simple de créer des chaînes de caractères qui incluent des variables ou des expressions en les plaçant entre accolades dans une chaîne préfixée par f.\n",
    "\n",
    "    #envoie de la requête et récupération du résultat en json\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    response = requests.get(url, params={\"query\": query}, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur lors de la requête SPARQL : {response.status_code}\")\n",
    "    return response.json()\n",
    "\n",
    "donnees = rechercher_oeuvres_wikidata(\"nuit\") #mot-test choisi = nuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30c25a-1fe5-42b2-abda-4eb407d5bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_requete = donnees[\"results\"][\"bindings\"]\n",
    "\n",
    "#voici à quoi ressemble une observation de notre table résultat :\n",
    "print(table_requete[0])\n",
    "\n",
    "noms_oeuvres = []\n",
    "#Observons la répartition du type d'oeuvre\n",
    "repartition = {}\n",
    "for item in table_requete:\n",
    "    type_oeuvre = item[\"typeLabel\"][\"value\"]\n",
    "    repartition[type_oeuvre] = repartition.get(type_oeuvre, 0) + 1\n",
    "    noms_oeuvres.append(item['oeuvreLabel']['value'])\n",
    "\n",
    "#print(repartition)\n",
    "#print(sorted(repartition.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "top_5 = dict(sorted(repartition.items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(top_5)\n",
    "#La répartition est équilibrée\n",
    "\n",
    "#Nous testons nos résultats avec des oeuvres que nous connaissons. Ces tests ont permis de tester la validité de notre requête.\n",
    "print(\"Voyage au bout de la nuit\" in noms_oeuvres)\n",
    "print(\"La Nuit étoilée\" in noms_oeuvres)\n",
    "print(\"Le Songe d'une nuit d'été\" in noms_oeuvres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3749741-405c-4588-8614-66a901381eb4",
   "metadata": {},
   "source": [
    "NP pour Louis : il faut prendre que les tops pour le camambert sinon mal certains seront trop petits et non visibles (exemple : peinture vs poème)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938332a-8849-4dbc-bdc0-02df9079b0ac",
   "metadata": {},
   "source": [
    "le résultat des titres d'oeuvres de la requête, si filtrer avec un fichier de stopwords français, permettera de calculer les co-occurences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea773b48-a916-4e3e-873b-308a974ce725",
   "metadata": {},
   "source": [
    "## Dimensions sociale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1707b4-1ecc-4c08-8794-c4bd69d05726",
   "metadata": {},
   "source": [
    "En commençant nos recherches, nous nous rendons compte que dans beaucoup de cas, le scraping n'est pas légal et l'utilisation d'API restreinte sur de nombreuses plateformes tels que Twitter. Avoir la possibilité de récupérer des données depuis [Reddit](https://www.reddit.com/dev/api/) serait déjà beaucoup.\n",
    "\n",
    "Voilà notre modèle \n",
    "[Google Books Ngram Viewer](https://books.google.com/ngrams/graph?content=incandescent&year_start=1800&year_end=2022&corpus=fr&smoothing=3)\n",
    "qui présente l'évolution dans le temps de l'usage d'un mot.\n",
    "\n",
    "![Google Books Ngram Viewer : mot \"incandescent\"](docs/google_books_ngram_viewer_incandescent.png)\n",
    "Nous souhaitons faire la même figure avec des réseaux sociaux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd45f5-1831-4e06-8f07-a6c0542b9bbf",
   "metadata": {},
   "source": [
    "### Twitter / X\n",
    "\n",
    "L'API v2 de Twitter propose une recherche sur mots-clés. Cependant l'accès nécessite un compte développeur validé et l'accès complet est payant, bien qu'un accès académique peut être demandé pour de la recherche. Nous optons donc pour une option plus simple et ouverte ([cf partie Open Data](###OpenData)), nous optons pour la récupération d'un jeu de données en ligne.\n",
    "Voici le jeu de données avec lequel nous travaillons : [French Tweets For Sentiment Analysis - 1.5 million tweets in French and their sentiment - Kaggle](https://www.kaggle.com/datasets/hbaflast/french-twitter-sentiment-analysis)\n",
    "\n",
    "Nous souhaitons obtenir de ce jeu de données les indicateurs suivants :\n",
    "- Fréquence du mot dans les tweets et\n",
    "- Score de popularité\n",
    "\n",
    "Le calcul du score de popularité se calcule de la manière suivante.\n",
    "Notre premier essai était la règle suivante : si le mot choisi apparaît autant de fois dans les tweets que le nombre de tweets de notre jeu de données, on considère qu'il mérite le score maximum et son score de popularité est alors de 100%. Ensuite, nous avons décidés de réechelonner car nous obtenions des scores trop faibles pour les mots les plus populaires tels que le mot \"**je**\". Puisqu'il est trop coûteux de calculer l'occurence maximale de chaque mot des nombreux tweets différents pour faire des règles relatives, nous décidons d'utiliser la règle suivante : $\\left(\\frac{\\text{fréquence\\_mot}}{\\frac{\\text{nombre\\_tweets}}{2}}\\right) \\times 100$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1a0ab-241f-46a0-9656-9ad2968a0113",
   "metadata": {},
   "source": [
    "Au moment du premier commit, nous nous rendons compte que nous ne pouvons pas travailler avec le dataset et le poster tel quel sur Github. En effet, GitHub refuse tout fichier > 100 Mo et déconseille les dépôts contenant plusieurs fichiers > 50 Mo. Une prémière opération consiste donc à créer une version diminuée du jeu de données de quelques Mo, passant de 1.5 millions de tweets à 50 000 tweets. Voici le code associé :\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Lire le fichier CSV\n",
    "df = pd.read_csv('data/french_tweets.csv')\n",
    "\n",
    "# renommer avec un nom de colonne plus clair\n",
    "df.rename(columns={\"text\": \"tweets\"}, inplace=True)\n",
    "\n",
    "# Sélectionner la colonne 'tweets' et prendre un echantillon de 50 000 tweets, choisi aléatoirement pour éviter les biais\n",
    "tweets = df['tweets'].apply(unidecode).sample(50000)\n",
    "\n",
    "# Convertir le texte en minuscules\n",
    "tweets = tweets.str.lower()\n",
    "\n",
    "# Exporter en CSV\n",
    "tweets.to_csv('data/french_tweets_mini.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c5df7-b98b-4541-ad48-f5244d75ce4d",
   "metadata": {},
   "source": [
    "Après observation du dataset, nous observons que beaucoup de personnes écrivent sans accent dans les tweets. Cela peut impacter la fréquence de mot et le score calculé selon la sensibilité à la casse ou le choix de l'encodage. Nous souhaitons donc normaliser (translittération) le mot en entrée et le texte des tweets également avec la librarie unidecode.\n",
    "\n",
    "> Unidecode transliterates any unicode string into the closest possible representation in ascii text\n",
    "\n",
    "En prenant en compte ce facteur, nous pouvons calculer nos indicateurs correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1772ebb7-7803-42d7-a796-0e7d17b86e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting unicode\n",
      "  Downloading unicode-2.9-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading unicode-2.9-py2.py3-none-any.whl (14 kB)\n",
      "Installing collected packages: unicode\n",
      "Successfully installed unicode-2.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbc5ab2c-1bd3-4b95-9372-585343fee044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularité du mot 'je' : 79.04 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>je ne suis donc pas d'humeur pour aujourd'hui!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>je dois mal de tete et mon nez plein comme s'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nettoyage a jamais! en esperant que je trouve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i am sam doit etre le film le plus triste de t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>- je voudrais si je pouvais y arriver. doit pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>matin! dommage, j'ai oublie le jeu pour l'hier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>je continuerai a croire aux nuages   avec cett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>hah, c'est bon, je laissais ta mere me battre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>desole, tu peux me faire un message texte jusq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>donc sur le travail. je ne peux pas croire que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19759 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets\n",
       "2         je ne suis donc pas d'humeur pour aujourd'hui!\n",
       "5      je dois mal de tete et mon nez plein comme s'i...\n",
       "6      nettoyage a jamais! en esperant que je trouve ...\n",
       "7      i am sam doit etre le film le plus triste de t...\n",
       "11     - je voudrais si je pouvais y arriver. doit pr...\n",
       "...                                                  ...\n",
       "49990  matin! dommage, j'ai oublie le jeu pour l'hier...\n",
       "49993  je continuerai a croire aux nuages   avec cett...\n",
       "49994     hah, c'est bon, je laissais ta mere me battre.\n",
       "49996  desole, tu peux me faire un message texte jusq...\n",
       "49999  donc sur le travail. je ne peux pas croire que...\n",
       "\n",
       "[19759 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "df = pd.read_csv('data/french_tweets_mini.csv') #le fichier csv a été réduit à 50000 observations pour permettre le stockage sur Github\n",
    "mot_normalise = unidecode(mot.lower()) #Les tweets du fichier csv ont subit les mêmes opérations (unicode + lower)\n",
    "\n",
    "nbr_tweets = df.shape[0]\n",
    "tweets_avec_mot = df[df[\"tweets\"].str.contains(mot_normalise)] \n",
    "\n",
    "frequence_mot = tweets_avec_mot.shape[0]\n",
    "\n",
    "#print(nbr_tweets, frequence_mot)\n",
    "\n",
    "#Si le mot choisi apparaît autant de fois dans les tweets que la moitié du nombre de tweets de notre dataset, on considère qu'il mérite le score maximum et son score de popularité est de 100%.\n",
    "score_popularite = (\n",
    "    (frequence_mot/ (nbr_tweets/2) )\n",
    "    *100\n",
    ")\n",
    "\n",
    "print(f\"Popularité du mot '{mot}' : {round(score_popularite, 2)} %\")\n",
    "#exemple : Popularité du mot 'je' : 79.04 %\n",
    "\n",
    "display(tweets_avec_mot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9e00b-79b8-46a5-aa54-fee6262102aa",
   "metadata": {},
   "source": [
    "La fonction `recherche_twitter()` du fichier python `twittter.py` permet ensuite d'exporter le fichier de la manière suivante\n",
    "\n",
    "```python\n",
    "from exportation_csv import exporter_donnees_csv\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\":[mot],\n",
    "    \"score_de_popularite\":[score_popularite]},\n",
    "    \"score_de_popularite.csv\"\n",
    ")\n",
    "\n",
    "tweets_avec_mot.to_csv(\"data/tweets_avec_mot.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d7f9d-3886-4975-98b3-e94ee4549d73",
   "metadata": {},
   "source": [
    "Voici comment nous appellerons cette fonction dans le code de notre interface web :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e28eb-1334-406f-ae64-d2a823fd5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter import recherche_twitter\n",
    "\n",
    "recherche_twitter(mot), recherche_twitter(mot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd0aff-e317-4558-9d9b-f9e64d3afc69",
   "metadata": {},
   "source": [
    "## API Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a9c5e-ee70-442d-a177-06c83562631f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Etapes que nous avons suivi pour utiliser l'API de Reddit\n",
    "\n",
    "- Créer une application : [Créer une application Reddit pour développeur](https://www.reddit.com/prefs/apps/)\n",
    "- Créer un fichier .env qui contient :\n",
    "```bash\n",
    "CLIENT_ID={identifiant de l application}\n",
    "CLIENT_SECRET={secret de l application}\n",
    "USER_AGENT=script:{nom de l application}:v1.0 (by u/{nom du profil reddit})\n",
    "```\n",
    "Cela évitera à Git de versionner notre fichier .env dans GitHub et de dévoiler nos codes d'application (NP faut-il le mettre à disposition pour le professeur ?)\n",
    "- installer la libraririe python\n",
    "> PRAW, an acronym for \"Python Reddit API Wrapper\", is a Python package that allows for simple access to Reddit's API. PRAW aims to be easy to use and internally follows all of Reddit's API rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9de770-26d0-4bd4-908b-22fbedd2a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960842f-b905-40e5-bab3-7c355d4f782e",
   "metadata": {},
   "source": [
    "- [Page Github de la librarire PRAW](https://github.com/praw-dev/praw)\n",
    "- [Documentation d'utilisation de l'API Reddit](https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html) \n",
    "\n",
    "![Reddit documentation : fonctionnement de la fonction search](docs/reddit_documentation_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93938b5-02d2-4e92-8bbb-17f301ed45ef",
   "metadata": {},
   "source": [
    "L'utilisation de l'API Reddit fonctionne et est la méthode la plus efficace pour récupérer les tops posts contenant le mot d'entrée et l'exporter en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072aefb3-f055-4165-908c-765026a9533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Charge les variables depuis .env\n",
    "\n",
    "#https://www.reddit.com/prefs/apps/\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "#documentation : https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html\n",
    "\n",
    "#rechercher des posts Reddit contenant le mot\n",
    "mot = \"créativité\"\n",
    "submissions = reddit.subreddit(\"all\").search(mot, sort=\"hot\", limit=100) #voir image documentation\n",
    "\n",
    "#...et récupération des données dans un dataframe pandas\n",
    "donnees = []\n",
    "for post in submissions:\n",
    "    donnees.append({\n",
    "        \"titre\": post.title,\n",
    "        \"texte\": post.selftext,\n",
    "        \"subreddit\": post.subreddit.display_name,\n",
    "        \"score\": post.score,\n",
    "        \"date\": pd.to_datetime(post.created_utc, unit=\"s\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(donnees)\n",
    "df.to_csv(f\"data/reddit_top_posts_avec_mot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f3f92-f499-489e-adcf-d2cead8dba44",
   "metadata": {},
   "source": [
    "Cependant, nous préférons, et ce pour des raisons de simplicité d'usage, de reproductibilité et d'autonomie vis-à-vis des services propriétaires, opter pour une solution ne nécessitant aucun identifiant d'API. En effet, l'utilisation de l'API Reddit officielle fonctionne bien, mais implique la création d'une application, la gestion de clés secrètes, et parfois des limites de requêtes ou des erreurs d'authentification (403, 429).\n",
    "\n",
    "Une alternative efficace consiste à utiliser un jeu de données librement accessible, tel que le fichier CSV hébergé sur GitHub : [reddit-top-2.5-million / france.csv](https://raw.githubusercontent.com/umbrae/reddit-top-2.5-million/master/data/france.csv). Ce fichier contient les publications les plus populaires du subreddit r/france, ce qui permet de mener les mêmes opérations d'analyse : recherche d'occurrence d'un mot, calcul de fréquence tout cela sans faire appel à une API ou à une authentification tierce.\n",
    "\n",
    "Le compromis réside dans la restriction thématique et temporelle : nous limitons nos analyses au seul subreddit r/france et aux publications populaires entre 2008 et 2015, ce qui peut biaiser l'étude de phénomènes récents. Néanmoins, cette approche a l'avantage de fonctionner de manière rapide, stable, et entièrement en local.\n",
    "\n",
    "Mais rien n'empêche d'ailleurs d'actualiser ce jeu de données : il est possible de créer soi-même un corpus Reddit via l'API, de scrapper les nouveaux flux RSS, ou encore de combiner plusieurs subreddits pertinents pour enrichir l'analyse. L'architecture du script est conçue pour rester modulaire, et permettre de remplacer facilement la source des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78bff8-0e9c-46d4-bc18-40d55b8e1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL du fichier CSV sur GitHub\n",
    "url = \"https://raw.githubusercontent.com/umbrae/reddit-top-2.5-million/master/data/france.csv\"\n",
    "\n",
    "# Lire le fichier CSV directement depuis l'URL\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bdc6f-74b3-4935-8570-ab60c5522c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"created_utc\"] = pd.to_datetime(df[\"created_utc\"]).dt.year #conversion objet temps puis extraction de l'année\n",
    "df[\"created_utc\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525517b2-8f45-4405-bcf7-d11c26aef07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtrer les lignes où 'selftext' ou 'title' contenant le mot\n",
    "df_filtre_mot = df[\n",
    "    df['selftext'].dropna().str.contains(mot.lower(), na=False) |\n",
    "    df['title'].str.contains(mot.lower(), na=False)\n",
    "] #rq : selftext contient des navalues qui empêchent le calcul si ces lignes ne sont pas exclus de la recherche sur le contenu des posts\n",
    "\n",
    "df_filtre_mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8888e8-a499-4a7e-912e-ab42b4d5adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtre_mot[\"ups\"].sum()\n",
    "df_filtre_mot[\"downs\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a303cd-a890-4be5-9a80-897466cb8e5b",
   "metadata": {},
   "source": [
    "## Dimension médiatique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcde36-b515-49ac-a458-111f9cb818d1",
   "metadata": {},
   "source": [
    "Wikipedia\n",
    "\n",
    "> Un flux RSS est créé à partir d'une page Web statique ou d'une base de données convertie en fichier XML à l'aide d'un script approprié.\n",
    "\n",
    "> Généralement, un flux RSS contient un titre (souvent celui d'un article), une description de l'article, et un lien vers le site concerné."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d299a1-ec34-4671-91e4-d53d6d127d90",
   "metadata": {},
   "source": [
    "Voici un [atlas des flux RSS français](https://atlasflux.saynete.net/atlas_des_flux_rss_fra_media.htm) dans lequel nous piochons. Les flux RSS possèdent tous la même structure. Il est donc simple de récupérer les informations souhaitées provenant de plusieurs sources différentes sans ne créer d'exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d623d-37ca-45cc-85b2-8aba84d4cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b72b54-6d36-4de6-b3ca-02e5bde97538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>lien</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eurovision 2025 : Louane a perdu, mais l’Europ...</td>\n",
       "      <td>Sun, 18 May 2025 16:10:00 +0000</td>\n",
       "      <td>Malgré une décevante septième place, la candid...</td>\n",
       "      <td>https://www.liberation.fr/international/europe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La mauvaise publicité d’Apple pour son nouvel ...</td>\n",
       "      <td>Fri, 10 May 2024 12:32:20 +0200</td>\n",
       "      <td>Le géant de la tech a retiré en catastrophe un...</td>\n",
       "      <td>https://www.courrierinternational.com/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Londres, capitale du gaming ?</td>\n",
       "      <td>Thu, 11 Apr 2024 16:57:52 +0200</td>\n",
       "      <td>Capitale souvent vantée pour sa richesse cultu...</td>\n",
       "      <td>https://www.courrierinternational.com/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titre  \\\n",
       "0  Eurovision 2025 : Louane a perdu, mais l’Europ...   \n",
       "1  La mauvaise publicité d’Apple pour son nouvel ...   \n",
       "2                      Londres, capitale du gaming ?   \n",
       "\n",
       "                              date  \\\n",
       "0  Sun, 18 May 2025 16:10:00 +0000   \n",
       "1  Fri, 10 May 2024 12:32:20 +0200   \n",
       "2  Thu, 11 Apr 2024 16:57:52 +0200   \n",
       "\n",
       "                                         description  \\\n",
       "0  Malgré une décevante septième place, la candid...   \n",
       "1  Le géant de la tech a retiré en catastrophe un...   \n",
       "2  Capitale souvent vantée pour sa richesse cultu...   \n",
       "\n",
       "                                                lien  \n",
       "0  https://www.liberation.fr/international/europe...  \n",
       "1  https://www.courrierinternational.com/article/...  \n",
       "2  https://www.courrierinternational.com/article/...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "\n",
    "def filtrer_articles_rss(url_flux, mot):\n",
    "    # Lecture du flux\n",
    "    flux = feedparser.parse(url_flux)\n",
    "    \n",
    "    # Liste pour stocker les articles filtrés\n",
    "    articles = []\n",
    "\n",
    "    # Parcours des articles\n",
    "    \n",
    "    #Les flux RSS possèdent tous la même structure\n",
    "    #Il est donc simple de récupérer les informations souhaitées provenant de plusieurs sources différentes sans ne créer d'exception\n",
    "    for entree in flux.entries:\n",
    "        #documentation W3C > The get() method returns the value of the item with the specified key + Optional : a value to return if the specified key does not exist.\n",
    "        titre = entree.get(\"title\", \"\")\n",
    "        description = entree.get(\"description\", \"\")\n",
    "        if (mot.lower() in titre.lower()) or (mot.lower() in description.lower()):\n",
    "            articles.append({\n",
    "                \"titre\": titre,\n",
    "                \"date\": entree.get(\"published\", \"\"),\n",
    "                \"description\": description,\n",
    "                \"lien\": entree.get(\"link\", \"\")\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "flux_list = [    \n",
    "    #depuis la liste initiale\n",
    "    \"https://www.lemonde.fr/actualite-medias/rss_full.xml\", #Le Monde #validé \"france\"\n",
    "    \"https://www.lemonde.fr/culture/rss_full.xml\", #Le Monde \n",
    "    \"https://www.liberation.fr/arc/outboundfeeds/rss/category/economie/medias/?outputType=xml\", #Libération #validé \"france\"\n",
    "    \"https://www.lefigaro.fr/rss/figaro_medias.xml\", #Le Figaro #validé \"france\"\n",
    "    \"https://bsky.app/profile/did:plc:2egpzsea27fru2vkrjgdw2ob/rss\", #MediaPart\n",
    "    #Le monde diplomatique\n",
    "    \"https://www.courrierinternational.com/feed/rubrique/ecrans/rss.xml\", #Courrier international #validé \"france\"\n",
    "    #Huffington Post\n",
    "    #Canard Enchaîné\n",
    "    \"https://www.humanite.fr/sections/medias/feed\", #L'humanité #validé \"france\"\n",
    "    \"https://www.nouvelobs.com/rss.xml\",  #L'obs #validé \"ukraine\"\n",
    "\n",
    "    #Autres\n",
    "    \"https://www.afp.com/fr/rss.xml\", #AFP #validé \"le\"\n",
    "\n",
    "    #Nous demandons ensuite à un LLM d'en générer le maximum sans ne regarder un par un la validité des liens\n",
    "    # Culture, musique, littérature\n",
    "    \"https://actualitte.com/feed\",                          # ActuaLitté (littérature)\n",
    "    \"https://www.lesinrocks.com/musique/feed/\",             # Les Inrocks Musique\n",
    "    \"https://www.rollingstone.fr/feed/\",                    # Rolling Stone France\n",
    "    \"https://www.telerama.fr/rss.xml\",                      # Télérama\n",
    "    \"https://www.franceculture.fr/rss.xml\",                 # France Culture\n",
    "\n",
    "    # Sciences humaines et philo\n",
    "    \"https://www.philomag.com/rss.xml\",                     # Philosophie Magazine\n",
    "    \"https://www.scienceshumaines.com/rss.xml\",             # Sciences Humaines\n",
    "\n",
    "    # Autres journaux généralistes\n",
    "    \"https://www.francetvinfo.fr/titres.rss\",               # France Info\n",
    "    \"https://www.radiofrance.fr/podcasts\",                  # Radio France\n",
    "    \"https://www.ouest-france.fr/rss-en-continu.xml\",       # Ouest-France\n",
    "    \"https://www.sudouest.fr/rss.xml\",                      # Sud-Ouest\n",
    "    \"https://www.ladepeche.fr/rss.xml\",                     # La Dépêche\n",
    "    \"https://www.midilibre.fr/rss.xml\",                     # Midi Libre\n",
    "    \"https://www.lavoixdunord.fr/rss.xml\",                  # La Voix du Nord\n",
    "    \"https://www.nicematin.com/rss\",                        # Nice Matin\n",
    "    \"https://www.20minutes.fr/rss/actu-france.xml\",         # 20 Minutes\n",
    "    \"https://www.france24.com/fr/rss\",                      # France 24\n",
    "\n",
    "    # Médias alternatifs et indépendants\n",
    "    \"https://www.bastamag.net/spip.php?page=backend\",       # Basta!\n",
    "    \"https://www.arretsurimages.net/rss/articles\",          # Arrêt sur Images\n",
    "    \"https://www.acrimed.org/spip.php?page=backend\",        # Acrimed\n",
    "\n",
    "    # Médias francophones internationaux\n",
    "    \"https://ici.radio-canada.ca/rss/4159\",                 # Radio-Canada\n",
    "    \"https://www.rfi.fr/fr/rss\",                            # RFI\n",
    "    \"https://www.tv5monde.com/rss/actualites\",              # TV5Monde\n",
    "\n",
    "    # Humour et satire\n",
    "    \"https://www.legorafi.fr/feed/\",                        # Le Gorafi\n",
    "\n",
    "    # Podcasts (audio)\n",
    "    \"https://feeds.acast.com/public/shows/61e6d2548e88e00012e13d0d\",  # Programme B (Binge)\n",
    "    \"https://rss.art19.com/le-code-a-change\"              # Le code a changé (Slate)\n",
    "    \n",
    "]\n",
    "\n",
    "df_total = pd.DataFrame()\n",
    "\n",
    "for flux in flux_list:\n",
    "    df_flux = filtrer_articles_rss(flux, mot)\n",
    "\n",
    "    #fusion du df obtenu avec le df total (concaténation ligne par ligne)\n",
    "    df_total = pd.concat([df_total, df_flux], ignore_index=True)\n",
    "\n",
    "df_total.to_csv(\"data/actualite_avec_mot.csv\")\n",
    "\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0471d-6400-4fda-9b92-5d8d19fc0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression formattage html avec bs4 ?\n",
    "#Pour verbatim\n",
    "#df_total[df_total[\"description\"].replace('\\xa0', '')].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5238c-ae05-4118-86f3-953523584201",
   "metadata": {},
   "source": [
    "```python\n",
    "from flux_rss import recup_articles\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7193c-a646-402e-9d65-5c97748800be",
   "metadata": {},
   "source": [
    "Image : ![mot Pulp, qui se trouve être dans l'actualité](docs/resultat_flux_rss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5e383-3711-407f-a0c9-13573bcebfab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Interface streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14e961-50c8-4e72-a76c-ce6cafb298f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Nous devons de passer par une interface Streamlit pour donner le choix de mot à l'utilisateur. Les questions qui se posent sont :\n",
    "\n",
    "- l'utilisateur doit il donner un mot existant (implique un fichier txt avec les mots français possibles)\n",
    "- faut il normaliser (oui),\n",
    "- gérer les bas de casse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ee79ba3-2a5c-4d6d-8b0b-86a8d3e460bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7812643-ed0f-40c7-8180-f7c5997199cc",
   "metadata": {},
   "source": [
    "[tutoriel streamlit](https://www.youtube.com/watch?v=D0D4Pa22iG0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dc3cf32-e3e9-481e-9fe7-ddb31bbb57fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1427193037.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m streamlit run interface_web.py\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m streamlit run interface_web.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b4f3b-0b8d-4696-9b93-be35d90c4c22",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
