{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b26a377-83e4-4d8e-8517-59ddf81dd863",
   "metadata": {},
   "source": [
    "# L'observatoire du mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78188147-b1c6-4cc0-94cf-6508e38a100c",
   "metadata": {},
   "source": [
    "à faire :\n",
    "\n",
    "- ~~créer un sous-dossier pour les fichiers csv ?~~\n",
    "- ~~les images ne fonctionnent pas dans github : / au lieu de \\\\~~\n",
    "- ~~créer la vérification synonymes.py et mettre à jour le rapport~~\n",
    "- ajout d'un readme -> intro de la fiche explicative\n",
    "- essayer l'interface web avec des mots qui n'existent pas\n",
    "- pourquoi le choix de scraper nos propores données ?\n",
    "- pourquoi pas de coloration syntaxique pour un chunc markdown sparql ?\n",
    "- docs -> ajouter un dossier wikidata\n",
    "- louis question -> que selectionne t on comme oeuvre dans Wikidata ?\n",
    "- json wikidata -> csv\n",
    "- à la fin nous integrerons les fichiers python en tant que fonctions d'un fichier utils.py\n",
    "- ajout traduction wikidata\n",
    "- interface web : avez-vous une application Reddit ?\n",
    "- test : effacer toute la data et vérifier qu'elle est créée correctement via le main ET l'interface WEB\n",
    "- tous les .py sont des fonctions\n",
    "- gestion des erreurs lorsque le mot n'existe pas\n",
    "- wikidata part :\n",
    "-     NP pour Louis : il faut prendre que les tops pour le camambert sinon mal certains seront trop petits et non visibles (exemple : peinture vs poème)\n",
    "-     le résultat des titres d'oeuvres de la requête, si filtrer avec un fichier de stopwords français, permettera de calculer les co-occurences.\n",
    "- images\n",
    "\n",
    "guidelines :\n",
    "- minimal core (dimension par dimension)\n",
    "\n",
    "questions :\n",
    "\n",
    "- Le rapport Excel dynamique doit contenir le plus de filtres possibles ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f37669",
   "metadata": {},
   "source": [
    "Liens importants : \n",
    "\n",
    "- cours : https://github.com/surybang/Reporting_openpyxl\n",
    "- template structure projet : https://github.com/surybang/template_usid0f\n",
    "- lien du repository : https://github.com/Aminata-Dev/L-observatoire-du-mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6d0a8-703a-40f7-8008-20c9527818e1",
   "metadata": {},
   "source": [
    "# Introduction – L'Observatoire du mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd221e-bf86-475f-834c-1dfcb9cda2c5",
   "metadata": {},
   "source": [
    "Qu'est-ce qu'un mot ? On croit souvent que comprendre un mot, c'est être capable de le définir. Pourtant, dans le langage ordinaire, les mots ne sont pas d'abord des objets de définition : ce sont des outils d'usage.\n",
    "\n",
    "Prenons pour exemple le mot « seum ». Se trouve-t-il dans le dictionnaire ? Peut-être. Mais même lorsqu'il y figure, la définition n'en fait que documenter un usage préexistant. Ce mot, comme tant d'autres, est d'abord appris par immersion, en l'entendant dans des situations précises puis en l'utilisant soi-même. On comprend un mot parce qu'on sait quand et comment l'utiliser – non parce qu'on est capable d'en réciter une définition. Les mots que l'on connait ne sont pas forcément des mots que l'on sait définir mais plutôt des mots que l'on sait utiliser. Comme l'écrivait Wittgenstein dans *Le Cahier Bleu* à ce sujet : « Nous sommes incapables de circonscrire clairement les concepts que nous utilisons ; non parce que nous ne connaissons pas leur vraie définition, mais parce qu'ils n'ont pas de vraie \"définition\". Supposer qu'il y en a nécessairement serait comme supposer que, à chaque fois que des enfants jouent avec un ballon, ils jouent en respectant des règles strictes.» (Wittgenstein, *Le Cahier bleu*, [25-26], trad. M. Goldberg et J. Sackur, Gallimard, p. 67-69).\n",
    "\n",
    "*L'Observatoire du mot* naît de ce constat : un mot est bien plus qu'une entrée dans un dictionnaire. Comprendre un mot, c'est plonger dans son usage vivant, ses contextes, ses résonances culturelles et sociales. L'utilisateur ne reçoit pas un portrait figé du mot de son choix, mais une matière vivante de l'explorer. L'Observatoire du mot est un outil hybride entre **dictionnaire augmenté** et cartographie culturelle pour toute personne curieuse de comprendre non seulement ce que signifie un mot mais également de connaître sa place dans le monde.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab062f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Objectifs et ambitions de *l'Observatoire du mot*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d7d89-046b-47cc-b41b-a55791d3422a",
   "metadata": {},
   "source": [
    "Notre objectif est de créer un programme Python permettant à l'utilisateur de saisir un mot, puis de générer un fichier Excel structuré, interactif et partageable. Ce fichier sera produit à l'aide de la librairie openpyxl. Voici la forme du ficher Excel que nous souhaitons produire :\n",
    "\n",
    "![Schéma](docs/schema_dashboard_observatoire_du_mot.png)\n",
    "\n",
    "Cette maquette de **tableau de bord** est inférée de la liste suivante présentant les « dimensions » d'un mot que nous aimerions explorer et visualiser :\n",
    "\n",
    "- **Dimension sémantique** : définitions / étymologie / synonymes / antonymes / citations célèbres / traductions.\n",
    "- **Dimension culturelle** : apparition du mot dans les titres d'œuvres d'art\n",
    "- **Dimension sociale** via les réseaux sociaux\n",
    "  - Récupérer les tops tweet/threads/commentaires/titres de vidéos/hashtags contenant le mot\n",
    "  - Co-occurrences : avec quels autres mots notre mot se retrouve-t-il le plus ? \n",
    "  - **Dimension statistique** : \n",
    "    - évolution de la fréquence d'apparition du mot\n",
    "\t- Donner un score de popularité (étoiles)\n",
    "- **Dimension médiatique** via médias.\n",
    "  - Retrouver les articles de l'actualité contenant ce mot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ade560",
   "metadata": {},
   "source": [
    "# Réflexions techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df2ce9-6f24-4208-a072-92834f90ac57",
   "metadata": {},
   "source": [
    "Pour rendre l'expérience utilisateur fluide, nous envisageons d'ajouter une interface Streamlit dans laquelle l'utilisateur pourra entrer le mot à explorer.\n",
    "Le programme se chargera ensuite de lancer l'ensemble de la pipeline, sans nécessiter de modifications manuelles dans le code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa716d88-41e9-49ed-8d7c-e87fd81c96a3",
   "metadata": {},
   "source": [
    "## Les données du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97145b6e-0c05-4b5f-b885-ea85ed5598c6",
   "metadata": {},
   "source": [
    "Mais cela soulève une question centrale : comment structurer la récupération des données pour que tout fonctionne dans un ensemble cohérent ? En effet, nous devons collecter des données provenant de nombreuses **sources hétérogènes** pour donner vie à L'Observatoire du mot. Nous avons identifié quatre méthodes principales pour les collecter :\n",
    "\n",
    "- Le flux RSS qui est une manière standardisée de recevoir les derniers contenus publiés sur un site comme un fil d'actualité. Par exemple les flux RSS de journaux comme Le Monde ou Mediapart peuvent nous informer en temps réel des nouveaux articles contenant un mot. Le flux RSS permet de satisfaire la dimension médiatique et statistique.\n",
    "- L'utilisation d'API (Application Programming Interface), interfaces propres aux développeurs qui permettent de demander à des plateformes leurs données via un programme. Exemples : API de Genius, Spotify, IMDB, Reddit, YouTube, Twitter, forums… On demande par exemple \"tous les posts contenant le mot X\" et la plateforme renvoie une réponse structurée en JSON. L'utilisation d'API permet de satisfaire la dimension sociale, culturelle et statistique. L'utilisation d'API de grandes plateformes telles que citées ci-dessus donne également plus de crédibilité au projet car cette approche offre un comparatif qui suscitera l'intérêt de l'utilisateur et rend la dimension statistique plus fiable car ce sont des plateformes utilisées par des milliards d'utilisateurs.\n",
    "- Le scraping est une technique consistant à extraire automatiquement du contenu visible sur une page web. Nous pensons réaliser du scraping sur des pages web statiques comme Wikitionary, CNRTL, Gallica ou CRISCO pour satisfaire la dimension sémantique. Le scraping est un processus simple qui ne demande pas d'identifiants et de création d'applications comme le nécessite l'utilisation d'une API. \n",
    "- Nous pourrons également récupérer et exploiter des jeux de données existants récupérés sur internet (notamment Kaggle ou Projet Gutenberg), pour explorer la dimension culturelle (par exemple jeu de données référençant les titres d'œuvres les plus connus ou récupération de corpus littéraire) et sociale (par exemple top tweets ou forums archivés).\n",
    "\n",
    "Une fois les données collectées, un autre enjeu est de les structurer de manière cohérente malgré leur diversité. Chaque dimension identifiée (sémantique, culturelle, sociale, médiatique) devra respecter un schéma commun afin d'être aisément manipulable en Python et visualisable dans le fichier Excel final.\n",
    "\n",
    "Pour sécuriser notre projet et pouvoir tester l'observatoire sans être dépendants des aléas du scraping ou des limites d'API, nous prévoyons de **constituer un jeu de données de secours**. Ce fichier contiendra un échantillon représentatif pour chaque dimension (tweets les plus connus, titres d'œuvres célèbres, quelques articles de presse, …). Notre projet pourra donc être utilisé en mode déconnecté, ou en cas de saturation de services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b23b4-6605-4668-b8fc-18f45fa71e8e",
   "metadata": {},
   "source": [
    "# Programmation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9108607-9d37-4a7c-a125-9a6a4067f570",
   "metadata": {},
   "source": [
    "## Structure du programme\n",
    "\n",
    "Exemple :\n",
    "Nous créons un fichier Python par données. Le fichier main sera chargé de faire appel au module d'importation des données et ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071bc92",
   "metadata": {},
   "source": [
    "## Choix du mot\n",
    "\n",
    "```python\n",
    "mot_entree = input(\"\\nEntre un mot de ton choix\\n@> \")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2954d28-8b24-41e4-b2ff-b16741244d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mot = \"imaginaire\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14487db4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dimensions sémantique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e2802-2ee4-4956-8b90-6d1474c8fa34",
   "metadata": {},
   "source": [
    "### Synonymes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159f8f1-e17f-458e-a87e-39d8822cb715",
   "metadata": {},
   "source": [
    "Le choix du site CRISCO (Centre de Recherche Inter-langues sur la Signification en Contexte) s'est imposé naturellement pour notre projet. Ce dictionnaire en ligne de synonymes maintenu par l’Université de Caen présente l'avantage d'être un site statique sans JavaScript ni contenu chargé dynamiquement, ce qui le rend simple à scraper avec des bibliothèques comme requests et BeautifulSoup. De plus, l'URL est directe et intelligible : il suffit d’y ajouter le mot voulu à la fin, sans passer par des identifiants numériques. Pour preuve, il suffit d'ajouter le mot du choix à l'url (et pas un code-index compliqué) pour accéder à la bonne page..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a22123-46e2-443f-8d96-9466f9c1ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://crisco4.unicaen.fr/des/synonymes/' + mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b4f0d-7637-423a-8589-3967a67ac97c",
   "metadata": {},
   "source": [
    "...et commencer à parser le contenu html de la page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f22e3e63-bee4-46c8-a634-2f721a6b934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "502995f5-36cb-4784-9b89-33238afd1aea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='crisco4.unicaen.fr', port=443): Max retries exceeded with url: /des/synonymes/imaginaire (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D8EA8C9610>, 'Connection to crisco4.unicaen.fr timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:199\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:789\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:490\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    489\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    492\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:466\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:1095\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:693\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    692\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    694\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:208\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[32m    209\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    210\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.host\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.timeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    211\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m: (<urllib3.connection.HTTPSConnection object at 0x000001D8EA8C9610>, 'Connection to crisco4.unicaen.fr timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:843\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    841\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='crisco4.unicaen.fr', port=443): Max retries exceeded with url: /des/synonymes/imaginaire (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D8EA8C9610>, 'Connection to crisco4.unicaen.fr timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectTimeout\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m page = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#bs4 permet de parser le contenu html d'une page \u001b[39;00m\n\u001b[32m      7\u001b[39m soupe = BeautifulSoup(page.text, features=\u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py:688\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    686\u001b[39m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, NewConnectionError):\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request=request)\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ResponseError):\n\u001b[32m    691\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request=request)\n",
      "\u001b[31mConnectTimeout\u001b[39m: HTTPSConnectionPool(host='crisco4.unicaen.fr', port=443): Max retries exceeded with url: /des/synonymes/imaginaire (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D8EA8C9610>, 'Connection to crisco4.unicaen.fr timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "#bs4 permet de parser le contenu html d'une page \n",
    "soupe = BeautifulSoup(page.text, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296f9e2-613a-43f5-af55-7a60cfa0c2f3",
   "metadata": {},
   "source": [
    "Nous souhaitons obtenir chaque synonyme associé au mot d'entrée et le score de popularité associé au synonyme. Ces éléments se présentent sous la forme suivante :\n",
    "\n",
    "![Tableau à scraper](docs/synonymes/tableau_synonymes_alambique.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cb606",
   "metadata": {},
   "source": [
    "![HTML derrière le tableau à scraper](docs/synonymes/inspection_tableau_synonymes_alambique.png)\n",
    "\n",
    "[Code source de la page](view-source:https://crisco4.unicaen.fr/des/synonymes/alambiqu%C3%A9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a86cb-d817-4d7d-95c9-b049e60578ca",
   "metadata": {},
   "source": [
    "Ce tableau contient toutes les informations que nous avons besoin de scraper. Nous ciblons d'abord la balise `<table>` et nous recherchons le contenu des balises `<a href>` qui contiennent les synonymes.\n",
    "\n",
    "Il faut veiller à cibler la basise `<table>`, sans cela nos premiers scraping du contenu des balises `<a href>` allait chercher tous les synonymes à l'ecran ailleurs que dans le tableau d'intérêt identifié plus haut et la somme des synonmymes était supérieure au nombre de lignes du tableau. Nous avons donc adapté notre code de la manière suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213c7d2-e40c-4f0f-a690-662f1528f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymes = []\n",
    "\n",
    "#modèle : <a href=\"/des/synonymes/balle\">balle</a>\n",
    "for balise_a in soupe.find('table').find_all('a', href=True):\n",
    "    if \"/des/synonymes/\" in balise_a['href']:\n",
    "        synonyme = (balise_a.text).replace('\\xa0', '')\n",
    "        #print(synonyme)\n",
    "        synonymes.append(synonyme)\n",
    "        \n",
    "print(synonymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d487115-8dd6-4269-bfba-f08f84ecfb13",
   "metadata": {},
   "source": [
    "Ensuite nous observons que la taille de la barre affichée grâce à `width` nous permet d'obtenir l'indicateur de score de proximité. Voici le modèle d'une balise contenant l'information de taille de la barre : `<hr style=\"height:6px;width:14px;color:#4040C0;background-color:#4040C0;text-align:left;margin-left:0\">`. Nous utilisons donc une expression régulière afin de récupérer cette information et de l'utiliser dans notre diagramme en bâtons. L'expression régulière est la suivante : `r'width:(\\d+)px'`\n",
    "\n",
    "(\\d+): Les parenthèses () sont utilisées pour capturer un groupe. \\d correspond à n'importe quel chiffre (0-9), et le + signifie \"un ou plusieurs\". Donc, \\d+ correspond à une séquence d'un ou plusieurs chiffres. Cette partie de l'expression régulière capture la valeur numérique de la largeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62934c0a-280a-4d44-804e-b5568a45691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modèle : <hr style=\"height:6px;width:14px;color:#4040C0;background-color:#4040C0;text-align:left;margin-left:0\">\n",
    "\n",
    "import re\n",
    "tailles_barre = []\n",
    "\n",
    "for hr in soupe.find_all('hr', style=True):\n",
    "    #print(hr[\"style\"])\n",
    "    \n",
    "    #extraction du nombre après \"width:\"\n",
    "    match = re.search(r'width:(\\d+)px', hr[\"style\"])\n",
    "\n",
    "    if match:\n",
    "        width = int(match.group(1))\n",
    "        #print(width)\n",
    "    \n",
    "    tailles_barre.append(width)\n",
    "\n",
    "print(tailles_barre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb146fb4-3694-45de-89bd-23d740f4d9de",
   "metadata": {},
   "source": [
    "### Vérification de la fiabilité du scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b71301-68b6-47dc-8046-601cec968fc4",
   "metadata": {},
   "source": [
    "Enfin, nous nous assurons que le nombre de score de proximité est bien le même que le nombre de synonyme trouvé grâce à l'instruction `assert len(tailles_barre) == len(synonymes)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e5346-2dd5-4e91-a5ac-f0b4c7022e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tailles_barre) == len(synonymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4748c85",
   "metadata": {},
   "source": [
    "Enfin, nous avons besoin de sauvegarder nos résultats afin de l'exploiter avec openpyxl. Nous décidons que chaque fichier comme `synonymes.py` doit générer un fichier csv pour le stockage et prêt à l'emploi pour l'utilisation par openpyxl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd904cd6-58ac-451d-93d2-f542287c45b8",
   "metadata": {},
   "source": [
    "## Exportation CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f3f85-99f4-41ac-a560-eb29136eebd0",
   "metadata": {},
   "source": [
    "### Questions d'optimisations\n",
    "\n",
    "La question qui se pose est la suivante : vaut-il mieux utiliser la librairie pandas ou la librarie csv pour le projet pour travailler et exporter les données? L'utilisation de la librarie csv disponible par défaut dans python permet de garder le programme ultra léger et plus rapide. En effet, la librarie pandas est plus lourde car elle inclut tout un écosystème data (ce qui implique des dépendances externes). Ces quelques milisecondes gagnées en utilisant une librarire ou une autre peuvent être cruciales si notre programme principal de génération du tableau de bord fait appel à plusieurs modules de scraping, requête API et flux RSS à la fois. Nous retenons tout de même la librairie pandas pour éviter de devoir inspecter les données à la main lorsque nous aurons à travailler avec données volumineuses comme les titres d'oeuvres d'art.\n",
    "\n",
    "### Demonstration\n",
    "Nous créons donc un ficher exportations_csv chargé d'exporter un dictionnaire python quelconque contenant les données de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b153e3-67de-465c-9f8c-a2ba547bebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def exporter_donnees_csv(donnees: dict[str, list], nom_fichier: str, index=False) -> None:\n",
    "    \"\"\"\n",
    "    Exporte un dictionnaire de données en fichier CSV.\n",
    "    \n",
    "    - donnees : dictionnaire {nom_colonne: liste_valeurs}\n",
    "    - nom_fichier : nom du fichier csv à créer (ex: \"synonymes.csv\")\n",
    "    \"\"\"\n",
    "\n",
    "    #création du chemin où sont stockés les données...\n",
    "    chemin_dossier = 'data'\n",
    "    chemin_complet = os.path.join(chemin_dossier, nom_fichier)\n",
    "    #... et création du sous-dossier data s'il n'existe pas\n",
    "    os.makedirs(chemin_dossier, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(donnees)\n",
    "    df.to_csv(chemin_complet, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7dd79-36b4-4222-8c15-5b3e8bcc4cf3",
   "metadata": {},
   "source": [
    "Voici un exemple d'exportation de données dans un format csv prêt à l'emploi pour openpyxl :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a5a88-fcb9-4013-b86c-ca54e26604e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymes_csv = {\n",
    "    \"mot\": [mot] *len(synonymes),\n",
    "    \"synonyme\": synonymes,\n",
    "    \"score_proximite_mot\": tailles_barre\n",
    "}\n",
    "\n",
    "exporter_donnees_csv(synonymes_csv, \"synonymes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aad765-c484-4cb2-8428-cd83e9ff7552",
   "metadata": {},
   "source": [
    "Chaque fichier python manipulant et traitant des données doit terminer par une instruction faisant appel à la fonction d'exportation. nous créons un module d'exportation et l'appelons dans les différents programmes de la manière suivante : \n",
    "```python \n",
    "from exportation_csv import exporter_donnees_csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620ca7c-d498-4aae-ad5b-e95ee1a607ef",
   "metadata": {},
   "source": [
    "Le code python permettant de récupérer les synonymes se trouve dans le fichier python `./synonymes.py`. Ce code possède en plus la gestion des erreurs grâce au try - except blocks.\n",
    "> The try block lets you test a block of code for errors.\n",
    "> The except block lets you handle the error.\n",
    "\n",
    "Notre fonction genère également le fichier CSV de synonymes dans le dossier `./data/` et retourner `False` si le site CRISCO est down ou si le mot n'est pas trouvé ou tout autre erreur qui empêcherait la génération du fichier CSV associé. \n",
    "\n",
    "Le code python permettant d'exporter un dictionnaire python en fichier csv se trouve dans le fichier `./exportation_csv.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed001c-53b9-48a9-8228-0e0b6fc12065",
   "metadata": {},
   "source": [
    "Il ne nous reste plus qu'à récupérer la fiche lexicale du mot. Ces éléments sont sa définition, sa prononciation et son étymologie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc03b06-5462-42f4-ad4f-0f6464dce78a",
   "metadata": {},
   "source": [
    "## Digression : l'Open Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9554c-f398-4ff1-a2c1-c168cc14bd86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Beaucoup de sites possédant des cookies wall [(exemple)](https://www.larousse.fr/dictionnaires/francais/alambiqu%C3%A9/2015) sont bannis de notre recherche. En effet, certaines plateformes, bien que riches en contenus, imposent des barrières à l'entrée (cookies wall, abonnements, API payantes, restrictions commerciales), limitant la possibilité d'exploration automatisée, de réutilisation ou de visualisation ouverte. Par exemple, des dictionnaires numériques grand public comme Larousse ou Le Robert verrouillent l'accès aux définitions via des dispositifs qui compliquent l'extraction de données à des fins de recherche ou d'analyse.\n",
    "\n",
    "En contraste, des ressources comme Wikidata, les flux RSS de la presse, ou encore des portails institutionnels comme le CNRTL (Centre National de Ressources Textuelles et Lexicales) incarnent l'esprit de l'open data. Ces plateformes favorisent la transparence de leurs structures de données, encouragent la réutilisation via des API ouvertes ou des formats interopérables et participent à la démocratisation de l'accès au savoir.\n",
    "\n",
    "Des plateformes comme le Centre National de Ressources Textuelles et Lexicales (CNRTL), issues de la recherche publique, offrent un accès libre à des définitions riches, étymologies, synonymes et exemples, sans publicité ni pistage. Dans la même optique, nous utiliserons Wikidata, une base de connaissances sémantique libre et collaborative, pour interroger dynamiquement nos œuvres à partir de mots en entrée. De la même façon, les flux RSS permettent de collecter légalement et en temps réel des titres et résumés d'articles d'actualité issus de médias variés, sans dépendre d'algorithmes opaques ou de systèmes de monétisation intrusifs.\n",
    "\n",
    "Ce type de ressource est essentiel pour bâtir des outils linguistiques, culturels ou éducatifs à destination du grand public.\n",
    "\n",
    "Ce choix de l'open data est donc à la fois une nécessité technique (ne pas dépendre d'écosystèmes fermés comme Twitter, dont les API sont devenues depuis peu inaccessibles) et une opportunité méthodologique (simplicité d'utilisation des données et formats standards) et un engagement politique pour une culture des communs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e319b-1196-4409-ab0b-98e31b8ec0d4",
   "metadata": {},
   "source": [
    "## Fiche lexicale\n",
    "\n",
    "Nous utilisons pour base de la création de fichiers csv constituant notre fiche lexicale pour leurs définitions complètes, leurs multiples exemples et leurs simplicité d'interface sans publicité les sites\n",
    "- [Centre National de Ressources Textuelles et Lexicales (CNRTL)](https://www.cnrtl.fr/definition/incandescent).\n",
    "- [Wikitionary](https://fr.wiktionary.org/wiki/incandescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c95f8-befd-480c-aaa0-f091a7095771",
   "metadata": {},
   "source": [
    "Nous créons un fichier python dédié à la génération d'une fiche lexicale nommé `fiche_lexicale.py`. Pour se faire, nous extrayons les données des deux sites cités précédemment pour obtenir une fiche lexicale complète.\n",
    "\n",
    "Voici notre code de scraping, qui suit la même logique que la récupération des synonymes, en gardant en tête qu'un bon scraping ne se fait pas avant d'avoir étudié le code source de la page en profondeur et avant d'avoir effectué les tests sur plusieurs cas pour repérer les erreurs de scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8dd65-4c11-4823-96ca-fd9bab11f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNRTL\n",
    "url = f\"https://www.cnrtl.fr/definition/{mot}\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "##### Définitions\n",
    "definitions = []\n",
    "for span in soup.find_all(\"span\", class_=\"tlf_cdefinition\"):\n",
    "    definitions.append(span.text)\n",
    "    \n",
    "print(\"\\nDéfinition :\")\n",
    "for d in definitions:\n",
    "    print(\"-\", d)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot] * len(definitions),\n",
    "    \"définition\":definitions},\n",
    "    \"definitions.csv\"\n",
    ")\n",
    "\n",
    "##### Étymologie\n",
    "etymologies = []\n",
    "for span in soup.find_all(\"span\", class_=\"tlf_ety\"):\n",
    "    etymologies.append(span.text)\n",
    "#print(etymologies)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot] * len(etymologies),\n",
    "    \"etymologie\":etymologies},\n",
    "    \"etymologies.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73576a7b-c359-4a10-a8ba-862177580cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikitionary\n",
    "url = f\"https://fr.wiktionary.org/wiki/{mot}\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "##### Citations\n",
    "#modèle : <bdi lang=\"fr\" class=\"lang-fr\"><i>Le lendemain, ils ne se voyaient pas. Les couples restaient enfermés chez eux, à la diète, écœurés, abusant de cafés noirs et de cachets <b>effervescents</b>.</i></bdi>\n",
    "citations = []\n",
    "for bdi in soup.find_all(\"bdi\", lang=\"fr\", class_=\"lang-fr\"):\n",
    "    if bdi.get(\"about\", \"\") != \"#mwt10\": #sinon prends des choses autres que des défintions dans la page\n",
    "        try:\n",
    "            citation = bdi.find(\"i\").text\n",
    "            #print(citation)\n",
    "            citations.append(citation)\n",
    "        except:pass #si pas de citations\n",
    "\n",
    "print(\"\\nCitations :\")\n",
    "for c in citations:\n",
    "    print(\"-\", c)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot] * len(citations),\n",
    "    \"citation\":citations},\n",
    "    \"citations.csv\"\n",
    ")\n",
    "\n",
    "##### Prononciation API\n",
    "#modèle prononciation : <span class=\"API\" title=\"Prononciation API\">\\e.fɛʁ.ve.sɑ̃\\</span></a>\n",
    "prononciation = mot\n",
    "for span in soup.find(\"span\", class_=\"API\",title=\"Prononciation API\"): #le premier est le français, les autres concernent les traductions\n",
    "    #print(span)\n",
    "    prononciation = span.text\n",
    "print(\"\\nPrononciation : \", prononciation)\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\"mot\": [mot],\n",
    "    \"prononciation\":[prononciation]},\n",
    "    \"prononciation.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46879701-1a1c-4c96-9fdd-77a553e925c9",
   "metadata": {},
   "source": [
    "Le code entier se trouve dans le fichier `./fiche_lexicale.py`. Pour extraire la fiche lexicale d'un mot, il faut appeler le fonction `recup_fiche_lexicale()` du fichier qui prend pour seul argument le mot d'entrée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd937d04-0c51-44b4-9ebd-07bc6b20acbe",
   "metadata": {},
   "source": [
    "## Dimension culturelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77353ee9-6917-42de-bfef-9e220fee5640",
   "metadata": {},
   "source": [
    "Le but est d'obtenir la distribution du mot d'entrée dans les titres d'oeuvres d'art afin d'en observer la répartition au sein du monde artistique. Nous souhaitons ensuite en faire un camembert (diagramme en secteurs).\n",
    "\n",
    "J'ai eu à travailler avec Wikidata au sein de mon entreprise dans le cadre de la fiabilisation d'une base de données interne. Naturellement l'idée d'obtenir des informations de titre d'oeuvres d'art de manière rapide et structurée ne pouvait pas se passer de Wikidata.\n",
    " \n",
    "Voici une description de l'outil que nous allons utiliser :\n",
    " \n",
    "> Wikidata est une base de connaissances libre, collaborative et multilingue créée par la Wikimedia Foundation. Elle centralise des données structurées sur une grande variété de sujets : personnes, œuvres d'art, lieux, concepts, événements, etc. Contrairement à Wikipédia, qui s'adresse aux humains sous forme d'articles encyclopédiques, Wikidata organise l'information de manière à être facilement lisible et interrogeable par des machines. Elle constitue une pierre angulaire du Web sémantique.\n",
    "\n",
    "> Chaque élément dans Wikidata possède un identifiant unique (par exemple Q42 pour Douglas Adams) et est décrit à l'aide d'énoncés structurés : propriétés (P31 pour \"instance de\", P1476 pour \"titre\", etc.) et valeurs (autres entités ou chaînes de caractères). Ces données sont interconnectées, multilingues et peuvent être exploitées à grande échelle.\n",
    "\n",
    "> Pour interroger Wikidata, on utilise le langage SPARQL (SPARQL Protocol and RDF Query Language), un langage standard conçu pour extraire des informations de bases de données RDF (Resource Description Framework). SPARQL fonctionne comme un équivalent de SQL pour les données du Web sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac1e08-1953-4694-ac95-04c8f7931c77",
   "metadata": {},
   "source": [
    "Voici un exemple de requête SPARQL similaire à notre objectif et trouvée dans la documentation de Wikidata. Cette requête permet de [trouver tous les artistes dont le genre artistique contient le mot 'rock'](https://query.wikidata.org/#%23Artistes%20dont%20le%20genre%20artistique%20contient%20le%20mot%20%27rock%27%0ASELECT%20DISTINCT%20%3Fhuman%20%3FhumanLabel%0AWHERE%0A%7B%0A%20%20%20%20VALUES%20%3Fprofessions%20%7Bwd%3AQ177220%20wd%3AQ639669%7D%0A%20%20%20%20%3Fhuman%20wdt%3AP31%20wd%3AQ5%20.%0A%20%20%20%20%3Fhuman%20wdt%3AP106%20%3Fprofessions%20.%0A%20%20%20%20%3Fhuman%20wdt%3AP136%20%3Fgenre%20.%0A%20%20%20%20%3Fhuman%20wikibase%3Astatements%20%3Fstatementcount%20.%0A%20%20%20%20%3Fgenre%20rdfs%3Alabel%20%3FgenreLabel%20.%0A%20%20%20%20FILTER%20CONTAINS%28%3FgenreLabel%2C%20%22rock%22%29%20.%0A%20%20%20%20FILTER%20%28%3Fstatementcount%20%3E%2050%20%29%20.%0A%20%20%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20%7D%0A%7D%0AORDER%20BY%20%3FhumanLabel%0ALIMIT%2050)\n",
    "\n",
    "```sparql\n",
    "#Artistes dont le genre artistique contient le mot 'rock'\n",
    "SELECT DISTINCT ?human ?humanLabel\n",
    "WHERE\n",
    "{\n",
    "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
    "    ?human wdt:P31 wd:Q5 .\n",
    "    ?human wdt:P106 ?professions .\n",
    "    ?human wdt:P136 ?genre .\n",
    "    ?human wikibase:statements ?statementcount .\n",
    "    ?genre rdfs:label ?genreLabel .\n",
    "    FILTER CONTAINS(?genreLabel, \"rock\") .\n",
    "    FILTER (?statementcount > 50 ) .\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}\n",
    "ORDER BY ?humanLabel\n",
    "LIMIT 50\n",
    "```\n",
    "\n",
    "Voici le site que j'utilise lors de mon travail en entreprise : [Wikidata Query Service](https://query.wikidata.org/). En effet Wididata met à disposition une interface simple en ligne permettant\n",
    "\n",
    "- d'écrire des requêtes SPARQL (plusieurs requêtes d'exemple sont disponibles sur le site);\n",
    "- de visualiser les données scraper dans une table et de faire des recherches de valeurs dedans;\n",
    "- d'exporter les données en JSON ou CSV.\n",
    "\n",
    "De plus, Wikidata propose un [tutoriel simple](https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial/fr) pour comprendre comment créer une requête pour obtenir les informations souhaitées grâce à la base de données Wikidata. Fait marrant, Wikidata commence son tutoriel par l'exemple d'une requête pour la recherche de toutes les oeuvres d'art pour expliquer un concept fondamental du schéma sujet - verbe - complément utilisé par la base de données Wikidata.\n",
    "\n",
    "![Tutoriel Wikidata : Classes et Instances](docs/tutoriel_wikidata.png)\n",
    "\n",
    "> Lorsque j'ai écrit ceci (octobre 2016), cette requête retournait 2615 résultats.\n",
    "\n",
    "Le nombre d'éléments ayant pour classe [\"oeuvre d'art\"](https://www.wikidata.org/wiki/Q838948) répertorié est maintenant de 34 319 le 4 juin et 34 320 le 5 juin 2025 !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c0143-5d02-401a-9050-332cd84e781e",
   "metadata": {},
   "source": [
    "Cependant et comme spécifié dans le tutoriel, requêter les oeuvres d'art n'est pas suffisant. Il faut également considérer les classes qui héritent de l'élément \"oeuvre d'art\", qu'ils soient enfants, petits-enfants ou petits petits enfants de l'élement oeuvre d'art. \n",
    "> Lorsque j'ai écrit ceci (octobre 2016), cette requête retrouvait 2615 résultats - évidemment, il y a plus d'œuvres d'art que cela ! Le problème est qu'il manque des éléments comme \"Autant en emporte le vent\", qui est seulement une instance de \"film\" et non de \"œuvre d'art\". \"film\" est une sous-classe d'\"œuvre d'art\", mais nous devons dire à SPARQL de prendre cela en compte lors de la recherche.\n",
    "\n",
    "La requête associée est la suivante :\n",
    "```sparql\n",
    "WHERE\n",
    "{\n",
    "  ?oeuvre wdt:P31/wdt:P279* wd:Q838948. # >* instance de n'importe quelle sous-classe d'une œuvre d'art : mais trop d'oeuvres -> la requête ne se termine pas\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "}\n",
    "```\n",
    "\n",
    "Mais cette requête pose problème. L'auteur de l'article exemple ecrit :\n",
    "> Je ne recommande pas d'exécuter cette requête. WDQS peut la gérer (tout juste), mais il est possible que votre navigateur se plante lors de l'affichage des résultats car ils sont très nombreux.\n",
    "\n",
    "C'est exactement ce qu'il se passe de notre côté.\n",
    "\n",
    "![Limite du temps de requête atteinte](docs/wikidata_limite_temps_atteinte.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf21ded-1fd4-483e-9f87-b407a445e920",
   "metadata": {},
   "source": [
    "En utilisant les requêtes montrées pécedemment (artistes dont le groupe contient le mot \"rock\" et la récupération de tous éléments instances d'oeuvre d'art), nous avons été capables de produire la requête suivante. Nous avons du pour cela procéder à une analyse des éléments que nous souhaitions recupérer afin de créer la requête qui nous convient. Cette analyse est expliquée longuement après cette requête."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a450500-77d6-410f-9116-58a315448be6",
   "metadata": {},
   "source": [
    "```sparql\n",
    "SELECT ?typeLabel (COUNT(?oeuvre) AS ?nbr)\n",
    "WHERE {\n",
    "  VALUES ?type { wd:Q3305213 } #peinture, oeuvres littéraires, film, album musical /// wd:Q3305213 wd:Q7725634 wd:Q11424 wd:Q482994\n",
    "\n",
    "  ?oeuvre wdt:P31 ?type. #instance de l'ensemble de classe selectionné ci-dessus\n",
    "\n",
    "  ?oeuvre rdfs:label ?oeuvreLabel. #Nous obtenons l'ensemble des libellés de nos oeuvres dans ?oeuvreLabel,\n",
    "  FILTER(LANG(?oeuvreLabel) = \"fr\"). #nous restreignons les libellés aux libéllés français\n",
    "  FILTER(CONTAINS(LCASE(?oeuvreLabel), \"nuit\")). #...puis nous vérifions que les libéllés contiennent le mot en questions\n",
    "\n",
    "  #?oeuvre wdt:P166 ?prix.  #uniquement les œuvres primées pour éviter un trop grand nombre de résultats\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". } #Nous créons les libéllés des variables\n",
    "\n",
    "}\n",
    "GROUP BY ?typeLabel\n",
    "LIMIT 450\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89111f69-9f3b-4039-b6e1-401c69bff212",
   "metadata": {},
   "source": [
    "Voici le cheminement de pensée qui nous as améné à trouver la requête ci dessus :\n",
    "\n",
    "Par exemple, je veux obtenir [_Le Livre de Sable_ de Jorge Luis Borges](https://www.wikidata.org/wiki/Q20761845) dans mes résultats lorsque je lance la requête avec le mot \"sable\", mais j'observe que chercher tous les éléments de Wikidata dont la classe est ou est sous-classe d'oeuvres d'art n'est pas suffisante.\n",
    "\n",
    "Je prends donc la classe [**œuvre littéraire**](https://www.wikidata.org/wiki/Q7725634) à laquelle ce livre appartient et l'ajoute à la liste des classes pour laquelle je cherche des éléments.\n",
    "Pour le mot \"nuit\", je souhaite obtenir _La nuit étoilée de Van Gogh_, [_Le Songe d'une nuit d'été de William Shakespeare_](https://www.wikidata.org/wiki/Q104871) et [_Voyage au bout de la nuit de Louis-Ferdinand Céline_](https://www.wikidata.org/wiki/Q105102304) dans le même résultat. Ces oeuvres qui ont des types d'oeuvres différents. J'observe que le premier est une instance de peinture qui elle-même est sous-classe d'oeuvre d'art. Notre requête réussira donc à récupérer cette oeuvre. Le deuxième est une instance d'oeuvre dramatique qui elle-même est sous-classe de d'[oeuvre littéraire](https://www.wikidata.org/wiki/Q7725634) qui n'est pas sous-classe d'[oeuvre](https://www.wikidata.org/wiki/Q386724) mais d'autres dérives : \n",
    "\n",
    "![Sous classe \"oeuvre litteraire\"](docs/wikidata_sous_classe_oeuvre_litteraire.png)\n",
    "\n",
    "Nous ajoutons donc aux côtés de la classe _oeuvre_ la classe [oeuvre littéraire](https://www.wikidata.org/wiki/Q7725634) dans notre recherche d'éléments. Cette méthode permet de récupérer [_Voyage au bout de la nuit de Louis-Ferdinand Céline_](https://www.wikidata.org/wiki/Q105102304) qui est une instance d'oeuvre littéraire. \n",
    "\n",
    "Ensuite, nous constatons qu'il n'y a aucun [film](https://www.wikidata.org/wiki/Q11424) dans la répartition de nos données. Nous ajoutons donc la [classe film](https://www.wikidata.org/wiki/Q11424) dans notre recherche. Il en va de même pour oeuvres musicales : nous cherchons une musique dans Wikidata, nous observons à quelle classe cet élément appartient et nous l'ajoutons à l'ensemble des classes cibles. Cependant, il n'y a pas beaucoup de chansons répértoriées. Pas autant que les albums de musique. Nous optons donc de choisir la classe \"album\" musical plutôt qu'oeuvres musicales.\n",
    "\n",
    "Ainsi, en inspectant nos données petit à petit, nous sommes capables de créer une requête équilibré et complète."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ab396-2a73-47dd-a6e4-255b8a5b5a3b",
   "metadata": {},
   "source": [
    "Durant les première tentatives de cette requête (NP mettre un lien), nous nous attendions à voir _La nuit étoilée (Cyprès et Villages)_ de Van Gogh. Or nous ne trouvions pas le tableau dans la liste des résultats, élément qui nous a mis sur la piste que nous devions faire attention à la **sensibilité à la casse** et nous avons rectifons ce problème en remplaçant `sparql FILTER CONTAINS(?peintureLabel, \"nuit\").` par `sparql FILTER(CONTAINS(LCASE(?peintureLabel), \"nuit\")).`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c3e4a-92de-44df-bb74-7bcf7139e3e4",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à voir si les résultats ne seront pas trop désequilibré d'un art à l'autre : on peut par exemple supposer que la littérature sera surreprésenté par rapport à la musique par exemple.\n",
    "\n",
    "Nous passons donc par python. En effet, nous sommes capables grâce à la librairie requests d'obtenir les résultats d'une requête SPARQL sur Wikidata. Nous déléguons donc la requête au programme python puis nous récupérons le resultat dans un dictionnaire afin d'observer la répartition des classes d'éléments trouvés selon différents paramètres. En effet, beaucoup de combinaisons et de classes différents sont possibles (: récupération des eléménts de classe X, instance et sous-classe de X, sous-classe et sous-sous-classe de la classe Y,...).\n",
    "\n",
    "- L'utilisation de python permet de rendre la requête modulable grâce à l'utilisation de f-string. Un f-string en Python est une manière simple de créer des chaînes de caractères qui incluent des variables ou des expressions en les plaçant entre accolades dans une chaîne préfixée par f. Par exemple, f\"WHERE = {mot}!\" insère la valeur de la variable mot directement dans la chaîne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d8435-dd58-45f8-8d34-53eace42a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Types d'œuvres : peinture, œuvre littéraire, film, album musical\n",
    "types_oeuvres = {\n",
    "    \"peinture\": \"wd:Q3305213\",\n",
    "    \"oeuvre_litteraire\": \"wd:Q7725634\",\n",
    "    \"film\": \"wd:Q11424\",\n",
    "    \"album_musical\": \"wd:Q482994\"\n",
    "}\n",
    "\n",
    "def requete_sparql_par_type(mot, type_wikidata):\n",
    "    \"\"\"\n",
    "    Recherche les titres d'œuvres contenant un mot via Wikidata (SPARQL).\n",
    "    Retourne une liste de titres.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Requête SPARQL : cherche des titres d'œuvres contenant le mot\n",
    "    #Nous rendons notre requête la plus modulable possible grâce à l'ajout de f-string\n",
    "    query = f\"\"\"\n",
    "    SELECT ?typeLabel (COUNT(?oeuvre) AS ?nbr)\n",
    "    WHERE {{\n",
    "      VALUES ?type {{ {type_wikidata} }} #peinture, oeuvres littéraires, film, album musical /// wd:Q3305213 wd:Q7725634 wd:Q11424 wd:Q482994\n",
    "\n",
    "      ?oeuvre wdt:P31 ?type. #instance de l'ensemble de classe selectionné ci-dessus\n",
    "      \n",
    "      ?oeuvre rdfs:label ?oeuvreLabel. #Nous obtenons l'ensemble des libellés de nos oeuvres dans ?oeuvreLabel,\n",
    "      FILTER(LANG(?oeuvreLabel) = \"fr\"). #nous restreignons les libellés aux libéllés français\n",
    "      FILTER(CONTAINS(LCASE(?oeuvreLabel), \"{mot.lower()}\")). #...puis nous vérifions que les libéllés contiennent le mot en questions\n",
    "      \n",
    "      #?oeuvre wdt:P166 ?prix.  #uniquement les œuvres primées pour éviter un trop grand nombre de résultats\n",
    "\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"fr\". }} #Nous créons les libéllés des variables\n",
    "\n",
    "    }}\n",
    "    GROUP BY ?typeLabel\n",
    "    LIMIT 450\n",
    "    \"\"\"\n",
    "    #Un f-string est une manière simple de créer des chaînes de caractères qui incluent des variables ou des expressions en les plaçant entre accolades dans une chaîne préfixée par f.\n",
    "\n",
    "    #envoie de la requête et récupération du résultat en json\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    response = requests.get(url, params={\"query\": query}, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur lors de la requête SPARQL : {response.status_code}\")\n",
    "    return response.json()\n",
    "\n",
    "#lancement de la requête pour chaque type\n",
    "resultats = []\n",
    "for label, qid in types_oeuvres.items():\n",
    "    json_data = requete_sparql_par_type(mot, qid)\n",
    "    bindings = json_data[\"results\"][\"bindings\"]\n",
    "    for res in bindings:\n",
    "        nbr_type = {\n",
    "            \"type\": res[\"typeLabel\"][\"value\"],\n",
    "            \"nombre\": int(res[\"nbr\"][\"value\"])\n",
    "        }\n",
    "        resultats.append(nbr_type)\n",
    "\n",
    "#et fusion des résultats dans un DataFrame\n",
    "df_resultats = pd.DataFrame(resultats)\n",
    "df_resultats.to_csv(\"data/repartition_types_oeuvres_art.csv\")\n",
    "df_resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b5978-9752-4d41-b832-4e50d3e97615",
   "metadata": {},
   "source": [
    "- L'utilisation de python et de la structure de données dictionnaire nous permet également de vérifier les résultats rapidement en effectuant la même opération : afficher la répartition des classes des éléments trouvés grâce à la requête. Lorsque nous ne récupérions pas uniquement le nombre d'éléments dans une catégorie avec le mot clé SQL `COUNT(), nous utilisions ce code python afin d'analyser nos résultats.\n",
    "\n",
    "\n",
    "```python\n",
    "donnees = rechercher_oeuvres_wikidata(\"nuit\") #mot-test choisi = nuit\n",
    "\n",
    "table_requete = donnees[\"results\"][\"bindings\"]\n",
    "\n",
    "#voici à quoi ressemble une observation de notre table résultat :\n",
    "print(table_requete[0])\n",
    "\n",
    "noms_oeuvres = []\n",
    "#Observons la répartition du type d'oeuvre\n",
    "repartition = {}\n",
    "for item in table_requete:\n",
    "    type_oeuvre = item[\"typeLabel\"][\"value\"]\n",
    "    repartition[type_oeuvre] = repartition.get(type_oeuvre, 0) + 1\n",
    "    noms_oeuvres.append(item['oeuvreLabel']['value'])\n",
    "    if 'http://www.wikidata.org/entity/' + item[\"oeuvre\"][\"value\"] == \"http://www.wikidata.org/entity/Q45585\":\n",
    "        print(item['oeuvreLabel']['value'])\n",
    "\n",
    "print(repartition)\n",
    "#print(repartition.items())\n",
    "top_5 = dict(sorted(repartition.items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(top_5)\n",
    "#print(sorted(repartition.items(), key=lambda x: x[1], reverse=True))\n",
    "print(\"Voyage au bout de la nuit\" in noms_oeuvres)\n",
    "print(\"La Nuit étoilée\" in noms_oeuvres)\n",
    "print(\"Le Songe d'une nuit d'été\" in noms_oeuvres)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea773b48-a916-4e3e-873b-308a974ce725",
   "metadata": {},
   "source": [
    "## Dimensions sociale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1707b4-1ecc-4c08-8794-c4bd69d05726",
   "metadata": {},
   "source": [
    "En commençant nos recherches, nous nous rendons compte que dans beaucoup de cas, le scraping n'est pas légal et l'utilisation d'API restreinte sur de nombreuses plateformes tels que Twitter. Avoir la possibilité de récupérer des données depuis [Reddit](https://www.reddit.com/dev/api/) serait déjà beaucoup.\n",
    "\n",
    "Voilà notre modèle \n",
    "[Google Books Ngram Viewer](https://books.google.com/ngrams/graph?content=incandescent&year_start=1800&year_end=2022&corpus=fr&smoothing=3)\n",
    "qui présente l'évolution dans le temps de l'usage d'un mot.\n",
    "\n",
    "![Google Books Ngram Viewer : mot \"incandescent\"](docs/google_books_ngram_viewer_incandescent.png)\n",
    "Nous souhaitons faire la même figure avec des réseaux sociaux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd0aff-e317-4558-9d9b-f9e64d3afc69",
   "metadata": {},
   "source": [
    "## Données Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a9c5e-ee70-442d-a177-06c83562631f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Voici les étapes que nous avons suivi pour utiliser l'API de Reddit\n",
    "\n",
    "- Créer une application : [Créer une application Reddit pour développeur](https://www.reddit.com/prefs/apps/)\n",
    "- Créer un fichier .env qui contient :\n",
    "```bash\n",
    "CLIENT_ID={identifiant de l application}\n",
    "CLIENT_SECRET={secret de l application}\n",
    "USER_AGENT=script:{nom de l application}:v1.0 (by u/{nom du profil reddit})\n",
    "```\n",
    "Cela évitera à Git de versionner notre fichier .env dans GitHub et de dévoiler nos codes d'application\n",
    "- installer la libraririe python\n",
    "> PRAW, an acronym for \"Python Reddit API Wrapper\", is a Python package that allows for simple access to Reddit's API. PRAW aims to be easy to use and internally follows all of Reddit's API rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9de770-26d0-4bd4-908b-22fbedd2a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960842f-b905-40e5-bab3-7c355d4f782e",
   "metadata": {},
   "source": [
    "- [Page Github de la librarire PRAW](https://github.com/praw-dev/praw)\n",
    "- [Documentation d'utilisation de l'API Reddit](https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html) \n",
    "\n",
    "![Reddit documentation : fonctionnement de la fonction search](docs/reddit_documentation_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93938b5-02d2-4e92-8bbb-17f301ed45ef",
   "metadata": {},
   "source": [
    "L'utilisation de l'API Reddit fonctionne et est la méthode la plus efficace pour récupérer les tops posts contenant le mot d'entrée et l'exporter en csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc21e34-11cf-4237-ace9-1f3e2c4ed6ef",
   "metadata": {},
   "source": [
    "```python\n",
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Charge les variables depuis .env\n",
    "\n",
    "#https://www.reddit.com/prefs/apps/\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "#documentation : https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html\n",
    "\n",
    "#rechercher des posts Reddit contenant le mot\n",
    "# mot = \"créativité\"\n",
    "submissions = reddit.subreddit(\"all\").search(mot, sort=\"hot\", limit=100) #voir image documentation\n",
    "\n",
    "#...et récupération des données dans un dataframe pandas\n",
    "donnees = []\n",
    "for post in submissions:\n",
    "    donnees.append({\n",
    "        \"titre\": post.title,\n",
    "        \"texte\": post.selftext,\n",
    "        \"subreddit\": post.subreddit.display_name,\n",
    "        \"score\": post.score,\n",
    "        \"date\": pd.to_datetime(post.created_utc, unit=\"s\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(donnees)\n",
    "df.to_csv(f\"data/reddit_top_posts_avec_mot.csv\")\n",
    "```\n",
    "\n",
    "(voir `reddit_via_api.py`, qui ne sera pas utilisé dans la génération du excel mais dispnible dans le repository github)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f3f92-f499-489e-adcf-d2cead8dba44",
   "metadata": {},
   "source": [
    "Cependant, nous préférons, et ce pour des raisons de simplicité d'usage, de reproductibilité et d'autonomie vis-à-vis des services propriétaires, opter pour une solution ne nécessitant aucun identifiant d'API. En effet, l'utilisation de l'API Reddit officielle fonctionne bien, mais implique la création d'une application, la gestion de clés secrètes, et parfois des limites de requêtes ou des erreurs d'authentification (403, 429).\n",
    "\n",
    "Une alternative efficace consiste à utiliser un jeu de données librement accessible, tel que le fichier CSV hébergé sur GitHub : [reddit-top-2.5-million / france.csv](https://raw.githubusercontent.com/umbrae/reddit-top-2.5-million/master/data/france.csv). Ce fichier contient les publications les plus populaires du subreddit r/france, ce qui permet de mener les mêmes opérations d'analyse : recherche d'occurrence d'un mot, calcul de fréquence tout cela sans faire appel à une API ou à une authentification tierce.\n",
    "\n",
    "Le compromis réside dans la restriction thématique et temporelle : nous limitons nos analyses au seul subreddit r/france et aux publications populaires entre 2008 et 2015, ce qui peut biaiser l'étude de phénomènes récents. Néanmoins, cette approche a l'avantage de fonctionner de manière rapide, stable, et entièrement en local.\n",
    "\n",
    "Mais rien n'empêche d'ailleurs d'actualiser ce jeu de données : il est possible de créer soi-même un corpus Reddit via l'API ou de combiner plusieurs subreddits pertinents pour enrichir l'analyse. L'architecture du script est conçue pour rester modulaire, et permettre de remplacer facilement la source des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a78bff8-0e9c-46d4-bc18-40d55b8e1613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>permalink</th>\n",
       "      <th>selftext</th>\n",
       "      <th>annee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.374528e+09</td>\n",
       "      <td>1iu8lq</td>\n",
       "      <td>France fuck ouais !</td>\n",
       "      <td>395</td>\n",
       "      <td>60</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/1iu8lq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.345977e+09</td>\n",
       "      <td>yuk2o</td>\n",
       "      <td>Je ne peux pas m'empêcher...</td>\n",
       "      <td>347</td>\n",
       "      <td>80</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/yuk2o/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.366299e+09</td>\n",
       "      <td>1clzj7</td>\n",
       "      <td>Une petite différence assez importante...</td>\n",
       "      <td>287</td>\n",
       "      <td>36</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/1clzj7...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.352949e+09</td>\n",
       "      <td>137wp4</td>\n",
       "      <td>Mon ami à New York</td>\n",
       "      <td>267</td>\n",
       "      <td>41</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/137wp4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.360585e+09</td>\n",
       "      <td>18b02o</td>\n",
       "      <td>À table !</td>\n",
       "      <td>253</td>\n",
       "      <td>35</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/18b02o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.357166e+09</td>\n",
       "      <td>15unaf</td>\n",
       "      <td>Public Cervix Announcement pour les redditrice...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/15unaf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1.354720e+09</td>\n",
       "      <td>14bot3</td>\n",
       "      <td>L'hiver sera rude à l'UMP</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/14bot3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.354651e+09</td>\n",
       "      <td>14a0oc</td>\n",
       "      <td>\"Le travail disparait, et c'est ce qu'on voula...</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/14a0oc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1.352474e+09</td>\n",
       "      <td>12wykz</td>\n",
       "      <td>Joseph Gordon Levitt qui chante du Brel</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/12wykz...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.351937e+09</td>\n",
       "      <td>12k5hh</td>\n",
       "      <td>Xavier Niel voit rouge</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>http://www.reddit.com/r/france/comments/12k5hh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      created_utc      id                                              title  \\\n",
       "0    1.374528e+09  1iu8lq                                France fuck ouais !   \n",
       "1    1.345977e+09   yuk2o                       Je ne peux pas m'empêcher...   \n",
       "2    1.366299e+09  1clzj7          Une petite différence assez importante...   \n",
       "3    1.352949e+09  137wp4                                 Mon ami à New York   \n",
       "4    1.360585e+09  18b02o                                          À table !   \n",
       "..            ...     ...                                                ...   \n",
       "995  1.357166e+09  15unaf  Public Cervix Announcement pour les redditrice...   \n",
       "996  1.354720e+09  14bot3                          L'hiver sera rude à l'UMP   \n",
       "997  1.354651e+09  14a0oc  \"Le travail disparait, et c'est ce qu'on voula...   \n",
       "998  1.352474e+09  12wykz           Joseph Gordon Levitt qui chante du Brel    \n",
       "999  1.351937e+09  12k5hh                             Xavier Niel voit rouge   \n",
       "\n",
       "     ups  downs                                          permalink selftext  \\\n",
       "0    395     60  http://www.reddit.com/r/france/comments/1iu8lq...      NaN   \n",
       "1    347     80  http://www.reddit.com/r/france/comments/yuk2o/...      NaN   \n",
       "2    287     36  http://www.reddit.com/r/france/comments/1clzj7...      NaN   \n",
       "3    267     41  http://www.reddit.com/r/france/comments/137wp4...      NaN   \n",
       "4    253     35  http://www.reddit.com/r/france/comments/18b02o...      NaN   \n",
       "..   ...    ...                                                ...      ...   \n",
       "995   26      4  http://www.reddit.com/r/france/comments/15unaf...      NaN   \n",
       "996   38     14  http://www.reddit.com/r/france/comments/14bot3...      NaN   \n",
       "997   27      5  http://www.reddit.com/r/france/comments/14a0oc...      NaN   \n",
       "998   28      5  http://www.reddit.com/r/france/comments/12wykz...      NaN   \n",
       "999   33      9  http://www.reddit.com/r/france/comments/12k5hh...      NaN   \n",
       "\n",
       "     annee  \n",
       "0     1970  \n",
       "1     1970  \n",
       "2     1970  \n",
       "3     1970  \n",
       "4     1970  \n",
       "..     ...  \n",
       "995   1970  \n",
       "996   1970  \n",
       "997   1970  \n",
       "998   1970  \n",
       "999   1970  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL du fichier CSV sur GitHub\n",
    "url = \"https://raw.githubusercontent.com/umbrae/reddit-top-2.5-million/master/data/france.csv\"\n",
    "\n",
    "# Lire le fichier CSV directement depuis l'URL\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df[\"annee\"] = pd.to_datetime(df[\"created_utc\"], utc=True, errors='coerce').dt.year #conversion objet temps puis extraction de l'année\n",
    "\n",
    "#suppression des colonnes inutiles\n",
    "#df.columns\n",
    "df = df[['created_utc', 'id', 'title', 'ups', 'downs', 'permalink', 'selftext', 'annee']]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525517b2-8f45-4405-bcf7-d11c26aef07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtrer les lignes où 'selftext' ou 'title' contenant le mot\n",
    "reddit_posts_avec_mot = df[\n",
    "    df['selftext'].dropna().str.contains(mot.lower(), na=False) |\n",
    "    df['title'].str.contains(mot.lower(), na=False)\n",
    "] #rq : selftext contient des navalues qui empêchent le calcul si ces lignes ne sont pas exclus de la recherche sur le contenu des posts\n",
    "\n",
    "reddit_posts_avec_mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a224b1-4462-444b-955f-0b41f36b29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exportation csv\n",
    "reddit_posts_avec_mot.to_csv(f\"data/reddit_posts_avec_mot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddc297-b345-4d03-a06c-f60eff84bace",
   "metadata": {},
   "source": [
    "voir `reddit.py` fonction `reddit_posts_avec_mot()` pour le code complet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d06b59-a930-4b3e-9182-00d2f098d3ee",
   "metadata": {},
   "source": [
    "NP ici pour graphique d'évolution openpyxl pour faire comme ngram. On ne choisit que Reddit parce que c'est déjà beaucoup en terme de données (flux rss ne présente que l'actuéalité récente et twitter fait payer l'api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03330ed-bddb-40df-9999-431cdd6aa484",
   "metadata": {},
   "source": [
    "### Twitter / X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5bc86-811e-4812-abfb-7a0a1346bd20",
   "metadata": {},
   "source": [
    "L'API v2 de Twitter propose une recherche sur mots-clés. Cependant l'accès nécessite un compte développeur validé et l'accès complet est payant, bien qu'un accès académique peut être demandé pour de la recherche. Nous optons donc pour une option plus simple et ouverte ([cf partie Open Data](#Digression-:-l'Open-Data)), nous optons pour la récupération d'un jeu de données en ligne.\n",
    "Voici le jeu de données avec lequel nous travaillons : [French Tweets For Sentiment Analysis - 1.5 million tweets in French and their sentiment - Kaggle](https://www.kaggle.com/datasets/hbaflast/french-twitter-sentiment-analysis)\n",
    "\n",
    "Nous souhaitons obtenir de ce jeu de données les indicateurs suivants :\n",
    "- Fréquence du mot dans les tweets et\n",
    "- Score de popularité, dont le caclul se fera dans la sous-partie [Score de popularité](#Score-de-popularite) ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1a0ab-241f-46a0-9656-9ad2968a0113",
   "metadata": {},
   "source": [
    "Au moment du premier commit, nous nous rendons compte que nous ne pouvons pas travailler avec le dataset et le poster tel quel sur Github. En effet, **GitHub refuse tout fichier > 100 Mo** et déconseille les dépôts contenant plusieurs fichiers > 50 Mo. Une prémière opération consiste donc à créer une version diminuée du jeu de données de quelques Mo, passant de 1.5 millions de tweets à 50 000 tweets. Voici le code associé :\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Lire le fichier CSV\n",
    "df = pd.read_csv('data/french_tweets.csv')\n",
    "\n",
    "# renommer avec un nom de colonne plus clair\n",
    "df.rename(columns={\"text\": \"tweets\"}, inplace=True)\n",
    "\n",
    "# Sélectionner la colonne 'tweets' et prendre un echantillon de 50 000 tweets, choisi aléatoirement pour éviter les biais\n",
    "tweets = df['tweets'].apply(unidecode).sample(50000)\n",
    "\n",
    "# Convertir le texte en minuscules\n",
    "tweets = tweets.str.lower()\n",
    "\n",
    "# Exporter en CSV\n",
    "tweets.to_csv('data/french_tweets_mini.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c5df7-b98b-4541-ad48-f5244d75ce4d",
   "metadata": {},
   "source": [
    "Après observation du dataset, nous observons que beaucoup de personnes écrivent sans accent dans les tweets. Cela peut impacter la fréquence de mot et le score calculé selon la sensibilité à la casse ou le choix de l'encodage. Nous souhaitons donc normaliser (translittération) le mot en entrée et le texte des tweets également avec la librarie unidecode.\n",
    "\n",
    "> Unidecode transliterates any unicode string into the closest possible representation in ascii text\n",
    "\n",
    "En prenant en compte ce facteur, nous pouvons calculer nos indicateurs correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772ebb7-7803-42d7-a796-0e7d17b86e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5ab2c-1bd3-4b95-9372-585343fee044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "df = pd.read_csv('data/french_tweets_mini.csv') #le fichier csv a été réduit à 50000 observations pour permettre le stockage sur Github\n",
    "mot_normalise = unidecode(mot.lower()) #Les tweets du fichier csv ont subit les mêmes opérations (unicode + lower)\n",
    "\n",
    "nbr_tweets = df.shape[0]\n",
    "tweets_avec_mot = df[df[\"tweets\"].str.contains(mot_normalise)] \n",
    "\n",
    "tweets_avec_mot.to_csv(\"data/tweets_avec_mot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b5d94-6fe9-49c5-aa1a-1c1a16c2dcc7",
   "metadata": {},
   "source": [
    "### Score de popularite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906c6c8-3027-413f-a1a0-c04e675691ec",
   "metadata": {},
   "source": [
    "Le calcul du score de popularité se calcule de la manière suivante pour Twitter :\n",
    "\n",
    "Notre premier essai était la règle suivante : si le mot choisi apparaît autant de fois dans les tweets que le nombre de tweets de notre jeu de données, on considère qu'il mérite le score maximum et son score de popularité est alors de 100%. Ensuite, nous avons décidés de réechelonner car nous obtenions des scores trop faibles pour les mots les plus populaires tels que le mot \"**je**\". Puisqu'il est trop coûteux de calculer l'occurence maximale de chaque mot des nombreux tweets différents pour faire des règles relatives, nous décidons d'utiliser la règle suivante :\n",
    "\n",
    "$$\n",
    "\\text{score de popularite}_{\\text{Twitter}} = \\left(\\frac{\\text{fréquence\\_mot\\_tweets}}{\\frac{\\text{nombre\\_tweets}}{2}}\\right) \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bd7e0-ab19-4345-adb6-ac0e3175b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence_mot_tweets = tweets_avec_mot.shape[0]\n",
    "#print(nbr_tweets, frequence_mot)\n",
    "\n",
    "#Si le mot choisi apparaît autant de fois dans les tweets que la moitié du nombre de tweets de notre dataset, on considère qu'il mérite le score maximum et son score de popularité est de 100%.\n",
    "score_popularite_twitter = (\n",
    "    (frequence_mot_tweets/ (nbr_tweets/2) )\n",
    "    *100\n",
    ")\n",
    "\n",
    "print(f\"Popularité du mot '{mot}' : {round(score_popularite_twitter, 2)} %\")\n",
    "#exemple : Popularité du mot 'je' : 79.04 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee60eb-82ea-4111-aa6c-da18ced486ee",
   "metadata": {},
   "source": [
    "Plutôt que de nous contenter d’un simple taux d'apparition du mot (nombre de posts contenant le mot divisé par le nombre total de posts), nous avons opté pour une approche plus qualitative, fondée sur l’engagement réel des utilisateurs. Sur Reddit, chaque post peut recevoir des upvotes (votes positifs) ou des downvotes (votes négatifs), ce qui permet d’évaluer non seulement la fréquence du mot mais aussi la réception sociale des publications qui le mentionnent.\n",
    "\n",
    "Nous avons donc défini un score de popularité comme le ratio entre le nombre total de upvotes et de downvotes sur l'ensemble des posts contenant le mot :\n",
    "\n",
    "$$\n",
    "\\text{score de popularite}_{\\text{Reddit}} = \\frac{\\sum \\text{ups}}{\\sum \\text{downs}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf6fd4-9be3-46fd-bce5-e77f04988ce3",
   "metadata": {},
   "source": [
    "Si le taux d'apparition brut (méthode utilisée pour les tweets) ne tient compte ni du contexte, ni de l'impact des publications, notre score basé sur les votes pondère chaque post selon sa réception par la communauté : un mot qui génère peu de publications mais fortement appréciées peut avoir un score élevé. Cela permet de détecter des mots **socialement valorisés**, et non simplement des mots fréquents ou neutres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a1c40-1b2f-416d-8d0b-c62d99da8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_popularite_reddit = round(reddit_posts_avec_mot[\"ups\"].sum() / reddit_posts_avec_mot[\"downs\"].sum(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307f9d9-7704-42c6-9883-d0daf79d9cf6",
   "metadata": {},
   "source": [
    "Nos deux scores de popularité n'ont pas la même échelle. Puisque nous ne pouvons pas générer de jauge Excel comme nous le souhaitions dans notre schéma d'origine, nous utiliserons seulement un code couleur\n",
    "(NP Louis code openpyxl score de popularité)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f699d20-b34b-4167-86c5-8ea673d9a626",
   "metadata": {},
   "source": [
    "### Exportation du score de popularité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a13d0-4cba-48d1-9fda-37376b819649",
   "metadata": {},
   "source": [
    "La fonction `recherche_twitter()` du fichier python `twitter.py` permet ensuite d'exporter le dataframe de la manière suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7422b7e-c399-4b08-b2f9-691e173b926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exportation_csv import exporter_donnees_csv\n",
    "\n",
    "exporter_donnees_csv(\n",
    "    {\n",
    "        \"mot\":[mot],\n",
    "        \"score_de_popularite_twitter\":[score_popularite_twitter],\n",
    "        \"score_de_popularite_reddit\":[score_popularite_reddit]\n",
    "    },\n",
    "    \"score_de_popularite.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9257fa-84f4-49ab-8495-63cb90007c22",
   "metadata": {},
   "source": [
    "## Conclusion dimension sociale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1621fb4-c8d8-41d5-9113-0e8f407e4ff1",
   "metadata": {},
   "source": [
    "Les top tweets et reddit posts enregistrés sous format csv plus haut nous seront utiles pour le verbatim, où nous afficherons des posts choisis au hasard parmi ces deux réseaux sociaux (NP code openpyxl verbatim)\n",
    "\n",
    "Voici comment nous appellerons obtiendrons ces données dans le code de notre interface web :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f2c27-7a0b-469a-ad56-396ca774917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter import recherche_twitter\n",
    "recherche_twitter(mot)[0] #l'index 1 permet renvoie le nombre d'observations de dataframe original. Cette donnée est utile pour la fonction du score de popularité pour le calcul du score de popularité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afc165-33bb-4343-b6ca-2dfecf25e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit import posts_reddit_r_france\n",
    "posts_reddit_r_france(mot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a42d9-845f-440b-b3b2-2fb4ae518323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_de_popularite import calcul_score_de_popularite\n",
    "calcul_score_de_popularite(mot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a303cd-a890-4be5-9a80-897466cb8e5b",
   "metadata": {},
   "source": [
    "## Dimension médiatique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcde36-b515-49ac-a458-111f9cb818d1",
   "metadata": {},
   "source": [
    "Les flux RSS offrent une alternative libre et ouverte pour recueillir des contenus médiatiques, tout en respectant l’architecture publique des sites d’actualités. Ils sont particulièrement adaptés à une veille continue et structurée autour de mots-clés ou thématiques spécifiques.\n",
    "\n",
    "Il est écrit sur Wikipedia :\n",
    "\n",
    "> Un flux RSS est créé à partir d'une page Web statique ou d'une base de données convertie en fichier XML à l'aide d'un script approprié.\n",
    "\n",
    "> Généralement, un flux RSS contient un titre (souvent celui d'un article), une description de l'article, et un lien vers le site concerné.\n",
    "\n",
    "La portée temporelle d’un flux RSS est en général limitée aux derniers articles publiés, souvent entre 20 et 50 entrées selon le média. Cela signifie que les flux permettent d’interroger une actualité immédiate, plutôt qu’un historique profond. \n",
    "\n",
    "NP Je me rends compte que notre dernier graphique n'est pas très pertinent parce que un mot en général n'apparaît pas souvent dans les articles du jour, même pour les sujets chaud. En fait la technique que j'utilise ne prend que les articles du jour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d299a1-ec34-4671-91e4-d53d6d127d90",
   "metadata": {},
   "source": [
    "Voici un [atlas des flux RSS français](https://atlasflux.saynete.net/atlas_des_flux_rss_fra_media.htm) dans lequel nous piochons. Les flux RSS possèdent tous la même structure. Il est donc simple de récupérer les informations souhaitées provenant de plusieurs sources différentes sans ne créer d'exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d623d-37ca-45cc-85b2-8aba84d4cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b72b54-6d36-4de6-b3ca-02e5bde97538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>lien</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biennale des Arts et de l’Océan de Nice : 11 e...</td>\n",
       "      <td>Sat, 14 Jun 2025 11:00:08 +0200</td>\n",
       "      <td>Photographes, plasticiens… A l’occasion de la ...</td>\n",
       "      <td>https://www.nouvelobs.com/culture/20250614.OBS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Les Veilleurs”, d’Ali Cherri : chimère, Bonne...</td>\n",
       "      <td>Fri, 13 Jun 2025 15:57:48 +0000</td>\n",
       "      <td>&lt;span class=\"field field--name-title field--ty...</td>\n",
       "      <td>https://www.philomag.com/articles/les-veilleur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bac philo 2025, l’ultime conseil : comment sur...</td>\n",
       "      <td>Fri, 13 Jun 2025 13:00:00 +0000</td>\n",
       "      <td>&lt;span class=\"field field--name-title field--ty...</td>\n",
       "      <td>https://www.philomag.com/articles/bac-philo-20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titre  \\\n",
       "0  Biennale des Arts et de l’Océan de Nice : 11 e...   \n",
       "1  “Les Veilleurs”, d’Ali Cherri : chimère, Bonne...   \n",
       "2  Bac philo 2025, l’ultime conseil : comment sur...   \n",
       "\n",
       "                              date  \\\n",
       "0  Sat, 14 Jun 2025 11:00:08 +0200   \n",
       "1  Fri, 13 Jun 2025 15:57:48 +0000   \n",
       "2  Fri, 13 Jun 2025 13:00:00 +0000   \n",
       "\n",
       "                                         description  \\\n",
       "0  Photographes, plasticiens… A l’occasion de la ...   \n",
       "1  <span class=\"field field--name-title field--ty...   \n",
       "2  <span class=\"field field--name-title field--ty...   \n",
       "\n",
       "                                                lien  \n",
       "0  https://www.nouvelobs.com/culture/20250614.OBS...  \n",
       "1  https://www.philomag.com/articles/les-veilleur...  \n",
       "2  https://www.philomag.com/articles/bac-philo-20...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "\n",
    "def filtrer_articles_rss(url_flux, mot):\n",
    "    # Lecture du flux\n",
    "    flux = feedparser.parse(url_flux)\n",
    "    \n",
    "    # Liste pour stocker les articles filtrés\n",
    "    articles = []\n",
    "\n",
    "    # Parcours des articles\n",
    "    \n",
    "    #Les flux RSS possèdent tous la même structure\n",
    "    #Il est donc simple de récupérer les informations souhaitées provenant de plusieurs sources différentes sans ne créer d'exception\n",
    "    for entree in flux.entries:\n",
    "        #documentation W3C > The get() method returns the value of the item with the specified key + Optional : a value to return if the specified key does not exist.\n",
    "        titre = entree.get(\"title\", \"\")\n",
    "        description = entree.get(\"description\", \"\")\n",
    "        if (mot.lower() in titre.lower()) or (mot.lower() in description.lower()):\n",
    "            articles.append({\n",
    "                \"titre\": titre,\n",
    "                \"date\": entree.get(\"published\", \"\"),\n",
    "                \"description\": description,\n",
    "                \"lien\": entree.get(\"link\", \"\")\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "flux_list = [    \n",
    "    #depuis la liste initiale\n",
    "    \"https://www.lemonde.fr/actualite-medias/rss_full.xml\", #Le Monde #validé \"france\"\n",
    "    \"https://www.lemonde.fr/culture/rss_full.xml\", #Le Monde \n",
    "    \"https://www.liberation.fr/arc/outboundfeeds/rss/category/economie/medias/?outputType=xml\", #Libération #validé \"france\"\n",
    "    \"https://www.lefigaro.fr/rss/figaro_medias.xml\", #Le Figaro #validé \"france\"\n",
    "    \"https://bsky.app/profile/did:plc:2egpzsea27fru2vkrjgdw2ob/rss\", #MediaPart\n",
    "    #Le monde diplomatique\n",
    "    \"https://www.courrierinternational.com/feed/rubrique/ecrans/rss.xml\", #Courrier international #validé \"france\"\n",
    "    #Huffington Post\n",
    "    #Canard Enchaîné\n",
    "    \"https://www.humanite.fr/sections/medias/feed\", #L'humanité #validé \"france\"\n",
    "    \"https://www.nouvelobs.com/rss.xml\",  #L'obs #validé \"ukraine\"\n",
    "\n",
    "    #Autres\n",
    "    \"https://www.afp.com/fr/rss.xml\", #AFP #validé \"le\"\n",
    "\n",
    "    #Nous demandons ensuite à un LLM d'en générer le maximum sans ne regarder un par un la validité des liens\n",
    "    # Culture, musique, littérature\n",
    "    \"https://actualitte.com/feed\",                          # ActuaLitté (littérature)\n",
    "    \"https://www.lesinrocks.com/musique/feed/\",             # Les Inrocks Musique\n",
    "    \"https://www.rollingstone.fr/feed/\",                    # Rolling Stone France\n",
    "    \"https://www.telerama.fr/rss.xml\",                      # Télérama\n",
    "    \"https://www.franceculture.fr/rss.xml\",                 # France Culture\n",
    "\n",
    "    # Sciences humaines et philo\n",
    "    \"https://www.philomag.com/rss.xml\",                     # Philosophie Magazine\n",
    "    \"https://www.scienceshumaines.com/rss.xml\",             # Sciences Humaines\n",
    "\n",
    "    # Autres journaux généralistes\n",
    "    \"https://www.francetvinfo.fr/titres.rss\",               # France Info\n",
    "    \"https://www.radiofrance.fr/podcasts\",                  # Radio France\n",
    "    \"https://www.ouest-france.fr/rss-en-continu.xml\",       # Ouest-France\n",
    "    \"https://www.sudouest.fr/rss.xml\",                      # Sud-Ouest\n",
    "    \"https://www.ladepeche.fr/rss.xml\",                     # La Dépêche\n",
    "    \"https://www.midilibre.fr/rss.xml\",                     # Midi Libre\n",
    "    \"https://www.lavoixdunord.fr/rss.xml\",                  # La Voix du Nord\n",
    "    \"https://www.nicematin.com/rss\",                        # Nice Matin\n",
    "    \"https://www.20minutes.fr/rss/actu-france.xml\",         # 20 Minutes\n",
    "    \"https://www.france24.com/fr/rss\",                      # France 24\n",
    "\n",
    "    # Médias alternatifs et indépendants\n",
    "    \"https://www.bastamag.net/spip.php?page=backend\",       # Basta!\n",
    "    \"https://www.arretsurimages.net/rss/articles\",          # Arrêt sur Images\n",
    "    \"https://www.acrimed.org/spip.php?page=backend\",        # Acrimed\n",
    "\n",
    "    # Médias francophones internationaux\n",
    "    \"https://ici.radio-canada.ca/rss/4159\",                 # Radio-Canada\n",
    "    \"https://www.rfi.fr/fr/rss\",                            # RFI\n",
    "    \"https://www.tv5monde.com/rss/actualites\",              # TV5Monde\n",
    "\n",
    "    # Humour et satire\n",
    "    \"https://www.legorafi.fr/feed/\",                        # Le Gorafi\n",
    "\n",
    "    # Podcasts (audio)\n",
    "    \"https://feeds.acast.com/public/shows/61e6d2548e88e00012e13d0d\",  # Programme B (Binge)\n",
    "    \"https://rss.art19.com/le-code-a-change\"              # Le code a changé (Slate)\n",
    "    \n",
    "]\n",
    "\n",
    "df_total = pd.DataFrame()\n",
    "\n",
    "for flux in flux_list:\n",
    "    df_flux = filtrer_articles_rss(flux, mot)\n",
    "\n",
    "    #fusion du df obtenu avec le df total (concaténation ligne par ligne)\n",
    "    df_total = pd.concat([df_total, df_flux], ignore_index=True)\n",
    "\n",
    "df_total.to_csv(\"data/actualite_avec_mot.csv\")\n",
    "\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dd2fe4e-f7f2-4acb-a24d-40bf7f852646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>lien</th>\n",
       "      <th>annee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biennale des Arts et de l’Océan de Nice : 11 e...</td>\n",
       "      <td>Sat, 14 Jun 2025 11:00:08 +0200</td>\n",
       "      <td>Photographes, plasticiens… A l’occasion de la ...</td>\n",
       "      <td>https://www.nouvelobs.com/culture/20250614.OBS...</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Les Veilleurs”, d’Ali Cherri : chimère, Bonne...</td>\n",
       "      <td>Fri, 13 Jun 2025 15:57:48 +0000</td>\n",
       "      <td>&lt;span class=\"field field--name-title field--ty...</td>\n",
       "      <td>https://www.philomag.com/articles/les-veilleur...</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bac philo 2025, l’ultime conseil : comment sur...</td>\n",
       "      <td>Fri, 13 Jun 2025 13:00:00 +0000</td>\n",
       "      <td>&lt;span class=\"field field--name-title field--ty...</td>\n",
       "      <td>https://www.philomag.com/articles/bac-philo-20...</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titre  \\\n",
       "0  Biennale des Arts et de l’Océan de Nice : 11 e...   \n",
       "1  “Les Veilleurs”, d’Ali Cherri : chimère, Bonne...   \n",
       "2  Bac philo 2025, l’ultime conseil : comment sur...   \n",
       "\n",
       "                              date  \\\n",
       "0  Sat, 14 Jun 2025 11:00:08 +0200   \n",
       "1  Fri, 13 Jun 2025 15:57:48 +0000   \n",
       "2  Fri, 13 Jun 2025 13:00:00 +0000   \n",
       "\n",
       "                                         description  \\\n",
       "0  Photographes, plasticiens… A l’occasion de la ...   \n",
       "1  <span class=\"field field--name-title field--ty...   \n",
       "2  <span class=\"field field--name-title field--ty...   \n",
       "\n",
       "                                                lien  annee  \n",
       "0  https://www.nouvelobs.com/culture/20250614.OBS...   2025  \n",
       "1  https://www.philomag.com/articles/les-veilleur...   2025  \n",
       "2  https://www.philomag.com/articles/bac-philo-20...   2025  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conversion objet temps puis extraction de l'année\n",
    "df_total[\"annee\"] = pd.to_datetime(df_total[\"date\"], errors='coerce', utc=True).dt.year\n",
    "df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5387a95-a442-4d44-9cba-1b91fb8a04e3",
   "metadata": {},
   "source": [
    "Nous souhaitons supprimer le formattage html grâce à bs4 pour la lisibilité et l'intégration au verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0471d-6400-4fda-9b92-5d8d19fc0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "avant_apres, textes_propres = [], []\n",
    "for description_html in df_total[\"description\"].values:\n",
    "    soup = BeautifulSoup(description_html, \"html.parser\")\n",
    "    texte_propre = soup.text.strip()\n",
    "    textes_propres.append(texte_propre)\n",
    "    avant_apres.append((description_html, texte_propre))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330092f-f837-4ea5-8313-0ff60cfe0a06",
   "metadata": {},
   "source": [
    "Ce n'est pas parfait, mais c'est beaucoup mieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f5fd8-6257-40d8-aec9-17c534d56f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "avant_apres_x = random.choice(avant_apres)\n",
    "try:print(avant_apres_x[0], '\\n=\\n', avant_apres_x[1])\n",
    "except:\"pas d'exemple disponible car pas d'articles trouvés\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226ad23-e2e0-454d-9dee-b21731d10452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[\"description\"] = textes_propres\n",
    "df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5238c-ae05-4118-86f3-953523584201",
   "metadata": {},
   "source": [
    "Le code total du flux rss doit être appelé de la manière suivante :\n",
    "```python\n",
    "from flux_rss import recup_articles\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5e383-3711-407f-a0c9-13573bcebfab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Interface web via Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8358a-0b5e-4124-a5f5-b27a61d0b9cf",
   "metadata": {},
   "source": [
    "## Gestion du mot en entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bffa53-0930-4234-86ee-9a6746433d07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Nous souhaitons passer par une web app grâce à [Streamlit](https://streamlit.io/) pour donner le choix du mot à l'utilisateur.\n",
    "\n",
    "Nous souhaitons n'autoriser que des mots existants pour pouvoir générer la fiche lexicale ou chercher les synonymes associés au mot sans ne générer d'erreur car le mot n'existe pas. Pour se faire, nous utilisons [une liste des mots du dictionnaire français](https://github.com/Taknok/French-Wordlist/blob/master/francais.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c62deb-888d-49d8-993d-d9764c666f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "# URL du fichier texte brut sur GitHub\n",
    "url = 'https://raw.githubusercontent.com/Taknok/French-Wordlist/master/francais.txt'\n",
    "\n",
    "# Récupérer le contenu du fichier\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    content = response.text\n",
    "    \n",
    "    mot_hasard = random.choice(content.splitlines())\n",
    "\n",
    "print(mot_hasard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ea1eb-88f4-4f62-81cd-01c271283825",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "La normalisation se fait à des niveaux locaux comme [la recherche dans les tweets](#Twitter-/-X) ou le nom de titre d'oeuvres mais n'a pas été généralisée car l'application de la fonction lower() sur des millions d'observations est souvent longue et peu utile dans notre cas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b450df7-4589-481a-84ae-93790e45d715",
   "metadata": {},
   "source": [
    "## Gestion des modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7812643-ed0f-40c7-8180-f7c5997199cc",
   "metadata": {},
   "source": [
    "Ensuite, nous regardons [ce simple tutoriel streamlit](https://www.youtube.com/watch?v=D0D4Pa22iG0) afin de réaliser une interface web minimale pour permettre de choisir le mot, afficher les données récupérées en ligne, faire les calculs, générer les fichiers csv et générer le fichier Excel final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ee79ba3-2a5c-4d6d-8b0b-86a8d3e460bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e1bf5-8897-4ae5-94fe-25882a960457",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m streamlit run interface_web.py\n",
    "```\n",
    "\n",
    "voir `main_interface_web.py`, fait office de programme principal dans notre projet car il appelle tous les programmes crées précedemment afin de créer le tableau de bord final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180120ef-1f8d-4343-8f28-d603d53eefbf",
   "metadata": {},
   "source": [
    "NP IMAGES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
